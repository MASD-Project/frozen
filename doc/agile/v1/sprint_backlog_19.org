#+title: Sprint Backlog 19
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Incorporate the relational model into Dogen.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-11-18 Mon 08:25]
| <75>                                                       |         |       |       |       |
| Headline                                                   | Time    |       |       |     % |
|------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                               | *87:04* |       |       | 100.0 |
|------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                    | 87:04   |       |       | 100.0 |
| Active                                                     |         | 87:04 |       | 100.0 |
| Edit release notes for previous sprint                     |         |       |  3:36 |   4.1 |
| Create a demo and presentation for previous sprint         |         |       |  1:39 |   1.9 |
| Time spent producing coding videos                         |         |       |  2:27 |   2.8 |
| Sprint and product backlog grooming                        |         |       |  6:09 |   7.1 |
| Remove test model sanitizer                                |         |       |  0:20 |   0.4 |
| Remove master headers                                      |         |       |  1:12 |   1.4 |
| Fix the issues with tests in reference model               |         |       |  3:53 |   4.5 |
| Update static strings to string views                      |         |       |  1:15 |   1.4 |
| ODB include directories is incorrect                       |         |       |  1:43 |   2.0 |
| Create a "modeling" report                                 |         |       |  4:56 |   5.7 |
| Move visual studio related options to top profile          |         |       |  0:17 |   0.3 |
| Fix incorrect message keyword when detecting ODB includes  |         |       |  0:29 |   0.6 |
| Meta-data keys are in the inverse order                    |         |       |  0:39 |   0.7 |
| Creating reference cycles produces strange errors          |         |       |  5:32 |   6.4 |
| Error on duplicate references                              |         |       |  0:26 |   0.5 |
| Generate model dependency graph                            |         |       |  1:55 |   2.2 |
| Make the building of the relational model conditional      |         |       |  0:23 |   0.4 |
| Make all transform contexts code-generated                 |         |       |  0:08 |   0.2 |
| Tracer numbering of dumped models is incorrect             |         |       |  0:26 |   0.5 |
| Fix compilation errors in c++ impl nightly                 |         |       |  0:24 |   0.5 |
| Add support for meta-data overrides in Dogen               |         |       |  5:16 |   6.0 |
| Some model names are too long for windows                  |         |       |  0:28 |   0.5 |
| Fake type in injection.json model                          |         |       |  0:18 |   0.3 |
| Add conditional logic for the inclusion of generated tests |         |       |  0:07 |   0.1 |
| Split generated tests from manual tests                    |         |       |  5:51 |   6.7 |
| Use clang9 and GCC 9 in nightly and CI                     |         |       |  0:01 |   0.0 |
| Full generation produces invalid code                      |         |       |  0:38 |   0.7 |
| Code coverage has not been updated since September         |         |       |  1:19 |   1.5 |
| Time spent fixing emacs issues                             |         |       |  1:54 |   2.2 |
| Consider creating a test build for all facets              |         |       |  1:53 |   2.2 |
| Move registrar into assets                                 |         |       |  5:29 |   6.3 |
| Upgrade to boost 1.70                                      |         |       | 10:00 |  11.5 |
| Setup laptop to work on dogen                              |         |       |  0:45 |   0.9 |
| Org-mode as a carrier format for modeling                  |         |       |  5:37 |   6.5 |
| Add relational tracing support                             |         |       |  9:39 |  11.1 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2019-06-03 Mon 12:59]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 16:01]--[2019-06-03 Mon 16:30] =>  0:29
    CLOCK: [2019-06-03 Mon 12:51]--[2019-06-03 Mon 12:59] =>  0:08
    CLOCK: [2019-06-03 Mon 09:51]--[2019-06-03 Mon 10:45] =>  0:54
    CLOCK: [2019-06-03 Mon 06:47]--[2019-06-03 Mon 08:52] =>  2:05
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.18, "Estação de Comboio"

#+begin_src markdown
![Nova estação de comboio e a antiga](https://gdb.voanews.com/957FFA4D-4D6B-49D0-B3C4-C5577701EEE8_w1597_n_r1_st.jpg)

_The new CFM train station, next to the old. Moçamedes, Namibe, Angola. (C) 2018 [Armando Chicoa (VOA)](https://www.voaportugues.com/a/autoridades-falam-em-neglig%C3%AAncia-no-acidente-de-comboios-no-namibe/4559078.html)._

**DRAFT: Release notes under construction**

# Introduction

At about three quarters of the planned commitment, this sprint was slightly shorter than usual. Nevertheless, it is still packed with intense work and exciting progress. The "meta-model all things" theme continues in full flow, and we just about reached the next great refactoring battlefront: the ```fabric``` namespaces in the C# and C++ generation models. Predictably, there are not many user facing stories, as the refactoring continues to gather steam.

# User visible changes

This section normally covers stories that affect end users, with the video providing a quick demonstration of the new features. As this sprint had only a very trivial user visible change (discussed below), we took the opportunity to demo a couple of existing features instead.

[![Sprint 1.0.18 Demo](https://img.youtube.com/vi/TkYQTW_jAGk/0.jpg)](https://youtu.be/TkYQTW_jAGk)

## Data directory clean up

For the last few sprints we have been chasing an elusive target: the removal of the assortment of non-model JSON files that have long lived in our ```data``` directory. If nothing else, anything with a name like "data" triggers immediately the "code smells" part of any developer's brain. With this sprint, we have finally achieved this milestone: the text templates that we use in the C++ and C# models have now been moved into the models themselves, with the addition of the text templates meta-modeling elements.

The change gave us the opportunity to rethink the approach from first principles. As a result, the ```data``` directory is no longer, and instead we now have only the ```library``` directory under the Dogen ```shared``` folder. It too will one day cease to exist, when we implement proper support for the PDMs (Platform Description Models) - but for the next three or four sprints it will continue to house the simplified version of the PDMs as they are currently implemented.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_18.org).

## Significant Internal Stories

There were four very significant stories this sprint, which we cover briefly below.

### Use generated static configurations in transforms

 First and foremost, consuming the majority of the sprint's resourcing, was the move towards using code generated static configurations. We started this work when we moved feature templates into the meta-model; it seemed only logical to start code-generating the C++ types to represent the dynamic configurations, as well as the "deserialisation" code that converted dynamic configurations to static configurations.

With this release we removed the majority of the hand-crafted uses of static configurations, making the code more readable. As an added bonus, It also means it's much easier to add new features to the code generator now: simply create a new instance of a ```masd::variability::feature_bundle``` modeling element, and add the required feature templates. While we were at it, we also cleaned up the way bundles were modeled, meaning we now have less boilerplate to add features and bundles are now more logically consistent.

 As an example of how feature bundles are used, here's how we declare the generalisation feature bundle:

```json
    {
      "name": "features::generalization",
      "documentation": "Features related to the generalization relationship.\n",
      "stereotypes": [
        "masd::variability::feature_bundle"
      ],
      "tagged_values": {
        "masd.variability.default_binding_point": "element",
        "masd.variability.archetype_location.kernel": "masd",
        "masd.variability.template_kind": "instance"
      },
      "attributes": [
        {
          "name": "masd.generalization.is_final",
          "type": "masd::variability::boolean",
          "documentation": "Whether to mark a type as final or not.\n",
          "tagged_values": {
            "masd.variability.qualified_name": "masd.generalization.is_final",
            "masd.variability.is_optional": "true"
          }
        },
        {
          "name": "masd.generalization.parent",
          "type": "masd::variability::text",
          "documentation": "Name of the parent of the current element.\n",
          "tagged_values": {
            "masd.variability.qualified_name": "masd.generalization.parent",
            "masd.variability.is_optional": "true"
          }
        }
      ]
    },

```

Users then make use of these features in their diagrams:

```
#DOGEN masd.generalization.is_final=true
#DOGEN masd.generalization.parent=some_package::some_type
```

We've already noticed how much quicker the development of new features has been since this new functionality has been added, so this is a great win.

### Make wale templates meta-model elements

As explained above, we have been chasing the "meta-modelisation" of all configuration files that lived in the data directory for a long time. Wale text templates were one of the most annoying cases, because they **really** did not belong in the data directory; after all, text templates are internal to the model that uses them, rather than visible to all users of the code generator.

With this release, we've finished adding support for a logic-less text template meta-modeling element, which represents the text template. We then moved the templates into their respective models, under the new ```templates``` directory. The name logic-less was chosen [to be close to the domain terminology](https://en.wikipedia.org/wiki/Mustache_(template_system)) but it perhaps yet another example of "domain overfitting": it seems it's more a source of confusion rather than enlightenment, as many users (and even domain experts!) are not familiar with the term. We will probably rename it to just "text templates".

![Logic-less templates](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/logic_less_templates_modeling_elements.png)

Interestingly, _in theory_, this change should have made possible for users to create their own text templates. However, _in practice_, it is of extremely limited value because:

- we do not yet have a stable API for the meta-modeling elements;
- nor do we expose these properly to the templates;
- nor do we have a proper logic-less templating engine such as one of the mustache-like clones that exist in C++.

However, it lays an important foundation for the work to come in this space and, though long in coming, the end goal in the area is now very well defined.

### Rename the ```coding``` model

Ever since we renamed our core model to ```coding``` we've been wondering if this was the right name. We've spent a fair bit of time wading through the literature in search of a fitting name, which would simultaneously reflect the domain terminology of [MDE](https://en.wikipedia.org/wiki/Model-driven_engineering), as well as clarifying our intent. We've finally settled on ```assets```, after reading the most enlightening review article by JM Jézéquel: ["Model-driven engineering for software product lines"](http://downloads.hindawi.com/journals/isrn.software.engineering/2012/670803.pdf).

The new name is also consistent with the fact that we intend to model both products and components within this meta-model, so hopefully the rename is future-proof, and - gasp - final. We have gone through some four or five names since Dogen's inception, so take that with a grain of salt.

### Start of Fabric clean-up

One of the most anticipated tasks has been moving the fabric meta-model elements from the C++ and C# generation models into the assets model (as it is now known). This sprint fired the starting shot in this race: we have addressed the modeling of forward declarations in C++'s fabric. These have now been made consistent with the modeling ideas in Fabric. Sadly, many more items remain: some 15 or so elements need to be re-thought and re-modeled, moved into assets and then all of the associated formatting code needs to be updated.

## Resourcing

As explained on the introduction, we've had around three quarters of the usual resourcing for this sprint, which was not ideal. On the plus side, over 77% of the sprint's total ask was spent on stories directly related to the sprint's mission, and just shy of 18% on process related work - with the release notes and demo consuming over 12% of that. Finally, we spent the remaining ~4% on spikes, mainly related to investigating the (many) test failures we're experiencing on Windows. Sadly no easy answers were to be found, so the investigation continues.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_pie_chart.jpg)

## Planning

The project plan has suffered a couple of major setbacks this sprint. First, predictably, the fabric clean up was not completed this sprint. In addition, it is now clear it will be much harder than what we had estimated, so its now set to cost us the entirety of the next sprint. In addition, the PDM work is significant and it had not yet been added to the project plan.

The updated plan is now as follows.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_18_resource_allocation_graph.png)

# Next Sprint

We shall focus on the Fabric clean-up for the entirety of the next sprint. It is likely that there will be some overrun, but we remain optimistic.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.18_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.18/dogen_1.0.18_amd64-applications.deb)
- [dogen-1.0.18-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.18/DOGEN-1.0.18-Darwin-x86_64.dmg)
- [dogen-1.0.18-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.18-Windows-AMD64.msi)

**Note 1**: we've made some slight improvements to the build duration, but in truth we're still desperately close to our 50 minutes allocation on Travis, and as such we're getting many red builds. This is not ideal, so next sprint we will probably need to start disabling some of the generated tests to lower the build times.

**Note 2:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this trivial.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+end_src markdown

- [[https://twitter.com/MarcoCraveiro/status/1135567734010523648][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6541333935140458497][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    CLOSED: [2019-06-03 Mon 12:59]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 10:46]--[2019-06-03 Mon 12:25] =>  1:39
    :END:

Time spent creating the demo and presentation.

*** COMPLETED Time spent producing coding videos                      :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 15:42]--[2019-11-15 Fri 15:47] =>  0:05
    CLOCK: [2019-11-15 Fri 10:35]--[2019-11-15 Fri 10:46] =>  0:11
    CLOCK: [2019-11-13 Wed 21:05]--[2019-11-13 Wed 21:20] =>  0:15
    CLOCK: [2019-11-13 Wed 08:18]--[2019-11-13 Wed 08:22] =>  0:04
    CLOCK: [2019-11-08 Fri 14:28]--[2019-11-08 Fri 15:00] =>  0:32
    CLOCK: [2019-11-08 Fri 09:50]--[2019-11-08 Fri 10:17] =>  0:27
    CLOCK: [2019-11-05 Tue 22:43]--[2019-11-05 Tue 23:06] =>  0:23
    CLOCK: [2019-11-05 Tue 21:30]--[2019-11-05 Tue 22:00] =>  0:30
    :END:

Story that captures time spent producing coding videos but not
actually doing any development related activities.

*** COMPLETED Sprint and product backlog grooming                     :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-11-13 Wed 08:36]--[2019-11-13 Wed 08:46] =>  0:10
    CLOCK: [2019-11-12 Tue 21:03]--[2019-11-12 Tue 21:22] =>  0:19
    CLOCK: [2019-11-08 Fri 15:39]--[2019-11-08 Fri 16:01] =>  0:22
    CLOCK: [2019-11-08 Fri 11:20]--[2019-11-08 Fri 11:32] =>  0:12
    CLOCK: [2019-11-06 Wed 07:40]--[2019-11-06 Wed 08:06] =>  0:26
    CLOCK: [2019-11-05 Tue 22:25]--[2019-11-05 Tue 22:42] =>  0:17
    CLOCK: [2019-11-01 Fri 11:01]--[2019-11-01 Fri 11:18] =>  0:17
    CLOCK: [2019-10-29 Tue 08:01]--[2019-10-29 Tue 08:37] =>  0:36
    CLOCK: [2019-10-28 Mon 17:40]--[2019-10-28 Mon 17:44] =>  0:04
    CLOCK: [2019-10-28 Mon 08:53]--[2019-10-28 Mon 08:58] =>  0:05
    CLOCK: [2019-10-27 Sun 12:41]--[2019-10-27 Sun 12:56] =>  0:15
    CLOCK: [2019-10-25 Fri 12:41]--[2019-10-25 Fri 13:12] =>  0:31
    CLOCK: [2019-10-24 Thu 17:30]--[2019-10-24 Thu 17:35] =>  0:05
    CLOCK: [2019-06-11 Tue 10:55]--[2019-06-11 Tue 11:15] =>  0:20
    CLOCK: [2019-06-04 Tue 09:36]--[2019-06-04 Tue 10:35] =>  0:59
    CLOCK: [2019-06-04 Tue 09:32]--[2019-06-04 Tue 09:35] =>  0:03
    CLOCK: [2019-06-03 Mon 19:46]--[2019-06-03 Mon 20:30] =>  0:44
    CLOCK: [2019-06-03 Mon 06:31]--[2019-06-03 Mon 06:46] =>  0:15
    CLOCK: [2019-06-03 Mon 06:21]--[2019-06-03 Mon 06:30] =>  0:09
    :END:

Updates to sprint and product backlog.

*** COMPLETED Remove test model sanitizer                             :story:
    CLOSED: [2019-06-03 Mon 16:51]
    :LOGBOOK:
    CLOCK: [2019-06-03 Mon 16:31]--[2019-06-03 Mon 16:51] =>  0:20
    :END:

We seem to have introduced testing to the reference implementation but
left the canned tests still there. There shouldn't be any reason to
keep these so remove them.

*** COMPLETED Remove master headers                                   :story:
    CLOSED: [2019-06-03 Mon 19:05]
    :LOGBOOK:
    CLOCK: [2019-06-04 Tue 09:21]--[2019-06-04 Tue 09:31] =>  0:10
    CLOCK: [2019-06-03 Mon 18:49]--[2019-06-03 Mon 19:01] =>  0:12
    CLOCK: [2019-06-03 Mon 18:28]--[2019-06-03 Mon 18:48] =>  0:20
    CLOCK: [2019-06-03 Mon 17:56]--[2019-06-03 Mon 18:12] =>  0:16
    CLOCK: [2019-06-03 Mon 16:52]--[2019-06-03 Mon 17:06] =>  0:14
    :END:

At present we are not making use of this feature, and it could even be
argued that it is not a feature that should be used; by having a
single header that includes all files we encourage unnecessary
inclusion, increasing build times. We had a use for this, which was
related to testing model types, but since we replace that with
generated tests, we no longer required it. Remove this feature.

Notes:

- actually, we left the test model sanitizer. Not clear why.

*** COMPLETED Code-generate variability feature templates             :story:
    CLOSED: [2019-06-03 Mon 20:23]

*Rationale*: implemented in the previous sprint.

Type templates are in effect features from a feature model. We need to
add UML support for features (e.g. add meta-model elements for them),
with code generation, and link them back to annotations.

In fact, we made a mistake by binding annotations so closely to
dogen. There are two distinct concerns here:

- the annotations library. This provides "typed support" on top of KVP
  infrastructure. The idea here is that users can define "fields" with
  "types" and retrieve information from those KVPs in a structured
  way. Instead of having to create their own validation
  infrastructure, they can rely on annotations to do all the hard work
  for them. As part of the field creation, ideas such as "scopes" and
  "archetype locations" emerge. However, these do not really belong to
  the domain of annotations; these are concepts that end users create
  and give them semantics. What annotations needs to be able to do is
  to allow the creation of arbitrary notions of "scopes" and
  "hierarchy". Basically, annotations could be a completely
  self-contained project with no dependencies and usable outside of
  dogen.
- the linkage between the annotations library and dogen. Here we can
  create metamodel elements to convey the input parameters needed to
  code generate the elements for the annotations library. In this
  sense, annotations is nothing more than a platform that the
  transforms leverage; it has nothing particularly special to do with
  dogen. It just so happens that dogen itself then makes use of
  annotations to supply metadata internally, but this is a mere
  coincidence.
- the linkage between stitch and annotations. In this view, stitch is
  yet another client of annotations, via dogen. Again, there is no
  reason why stitch needs to have any dependency on dogen, other than
  annotations. In this sense, features such as licences and other
  boilerplate must be supplied as KVP parameters into stitch, without
  it directly depending in formattables. In addition, the fact that
  stitch generates c++ is also a coincidence. We could have a
  parameter that configures stitch and generate say C#.

Interestingly, in this sense we could then say that both stitch and
annotations are stand alone libraries generated using dogen, and then
in turn consumed by dogen. This could be done as packages by means of
vcpkg. And of course, stitch could then use a proper templating engine
instead of wale (another vcpkg dependency).

Finally, the logical conclusion is that dogen can use *any* of a
number of templating engines. The parameters to the engine are
supplied using KVPs (by means of annotation). There is a generic
metamodel element representing the binding to templating, and one of
its parameters is the templating engine. These are bound to the dogen
binary at compile time. End users can also make use of this mechanism,
for any of the available facets. This means that where we supply
=formatting_style=, we should really reflect the templating
engine. And then, all parameters with a known prefix, say:

: masd.templating.ENGINE.X=Y

Are supplied as parameters to the engine. These may need to take into
account facets as well, so that we can bind each facet to a different
template and supply different parameters.

Notes:

- one really useful feature would be to bind an enumeration to a
  string field, such that we'd automatically convert the string into a
  valid value of the enumeration (or throw).

*Previous Understanding*

Tasks:

- create a meta-model element for type templates. Add container in
  exomodel for it. Name: =yarn::annotation_type_template=?
- add frontend support for the type template element.
- add a transform that reads all the meta-data from type templates and
  populates the yarn element of the type template. Add this transform
  to the exomodel transforms, at the end of the chain (e.g. after
  annotations).
- create a meta-model element for the initialiser of type templates,
  made up of all type templates in the model. Add a container of
  initialiser in endomodel.
- add a transform that moves all of the type templates into the
  initialiser. This can be done as part of the exomodel to endomodel
  transform. Or maybe we should have a stand alone transform, and the
  final transform simply ignores type templates.
- create a registrar in annotations that registers type templates.
- create a stitch template for the initialiser, taking the registrar
  as an argument, and registering all type templates.
- add all type templates to all models, and generate the type
  initialisers.
- hook the type initialisers to the initialisers.
- change type group repository to initialise from the registrar.
- delete all type groups JSON and hydrator and related code.

Merged stories:

*Initialisation of meta-data*

At present we are reading meta-data files for every transformation. In
reality, it makes no sense to allow the meta-data files to change
dynamically, because the consumers of the meta-data are hard-coded. So
it would make more sense to treat them as a initialisation step. This
will make even more sense when we code-generate the types instead of
using JSON. Then we can hook up the generated code to the
initialisers.

*** COMPLETED Fix the issues with tests in reference model            :story:
    CLOSED: [2019-06-19 Wed 16:48]
    :LOGBOOK:
    CLOCK: [2019-06-18 Tue 20:02]--[2019-06-18 Tue 23:55] =>  3:53
    :END:

It seems when we added the tests in the test model, we did not enable
them for all models: we skipped a few, probably because we started
seeing lots of compilation errors. However, now that we need to test
serialisation with the new registrar, we need those tests. We need to
go back and figure out why the tests where failing and fix them.

Notes:

- immutability issues: some tests cannot run if a type is immutable
  (e.g. assignment, etc).
- issues with the new tests facet directory and destination.

*** CANCELLED Update static strings to string views                   :story:
    CLOSED: [2019-09-05 Thu 11:05]
     :LOGBOOK:
     CLOCK: [2019-09-05 Thu 10:50]--[2019-09-05 Thu 11:04] =>  0:14
     CLOCK: [2019-09-04 Wed 19:20]--[2019-09-04 Wed 19:47] =>  0:27
     CLOCK: [2019-09-04 Wed 18:45]--[2019-09-04 Wed 19:19] =>  0:34
     :END:

 Now we're on C++17 we can start making use of its new features. One
 low hanging fruit is string view. We use static strings quite a lot
 for logging etc. We can just replace these with string views.

 Example:

 : #include <string_view>
 : constexpr std::string_view foo("abc");

 Problems:

 - cannot do XML text reader because we do not have a good way to
   convert string_view to cstr. See [[https://stackoverflow.com/questions/48081436/how-you-convert-a-stdstring-view-to-a-const-char][How you convert a std::string_view
   to a const char*?]]

 Links:

 - [[https://www.bfilipek.com/2018/10/strings17talk.html][Let's Talk About String Operations in C++17]]
 - [[https://developercommunity.visualstudio.com/content/problem/24487/constexpr-stdstring-view-from-string-literal.html][constexpr std::string_view from string literal]]
 - [[https://www.reddit.com/r/cpp/comments/cw35kk/best_practices_for_efficient_string_constants/][Best practices for efficient string constants]]

*** COMPLETED ODB include directories is incorrect                    :story:
    CLOSED: [2019-10-02 Wed 16:53]
    :LOGBOOK:
    CLOCK: [2019-10-02 Wed 15:46]--[2019-10-02 Wed 16:53] =>  1:07
    CLOCK: [2019-10-02 Wed 13:20]--[2019-10-02 Wed 13:56] =>  0:36
    :END:

With the upgrade to vcpkg ODB, we have broken ODB generation. The
problem is that we rely on the export of =ODB_INCLUDE_DIRS=, but this
no longer happens as the include directories are set by vcpkg. The
right solution is to rely only on the global includes.

In fact the right solution is to set globally a
=ODB_EXECUTABLE_GLOBAL_ARGS= and reuse that in each generated file. We
should also ensure this variable is defined and issue a message
explaining the problem.

Links:

- [[https://stackoverflow.com/questions/47475731/cmake-include-directories-for-custom-target-type/58200691#58200691][CMake include_directories for custom target type]]
- [[https://cmake.org/cmake/help/v3.3/command/target_include_directories.html][CMake manual: target_include_directories]]

*** CANCELLED Create a "modeling" report                              :story:
    CLOSED: [2019-10-26 Sat 16:02]
    :LOGBOOK:
    CLOCK: [2019-10-25 Fri 22:22]--[2019-10-25 Fri 23:40] =>  1:18
    CLOCK: [2019-10-25 Fri 15:21]--[2019-10-25 Fri 16:41] =>  1:20
    CLOCK: [2019-10-25 Fri 13:13]--[2019-10-25 Fri 14:55] =>  1:42
    CLOCK: [2019-10-24 Thu 17:50]--[2019-10-24 Thu 18:13] =>  0:23
    CLOCK: [2019-10-24 Thu 17:36]--[2019-10-24 Thu 17:49] =>  0:13
    :END:

*Rationale*: we will address this via the relational model instead.

At present when we introduce a new modeling element and things stop
working, its very difficult to understand why. The problem could be
any where in the pipeline, and looking through the logs and the
transform reports doesn't make the task easier. The information is
there but the problem is knowing where to look. The ideal scenario is
to have a relational model describing all working within dogen, but
that is a lot of work. One quicker way of getting some of this
information is to create a "modeling report". This would be in
org-mode format and have a hierarchical structure like so:

- run:
  - start time
  - command line options
- models:
  - name of the model
  - input language, output languages.
  - path to the model
  - global enablement properties
  - type: target or reference.
- dia elements:
  - dia object name
  - dia object type as tag.
  - stereotypes
  - transforms: processed and skipped. These are groups of transforms
    that processed or skipped the element.
  - assets: asset meta-model elements for this object.
    - transforms: processed and skipped.
    - artefacts
      - flag of enabled or disabled
      - path
      - transforms: processed and skipped.

Notes:

- we already have start and end transform/chain in tracer. We just
  need a way to mark a type as processed at the end of a transform,
  else it should be marked as ignored/skipped. We can use the
  qualified name for this; e.g. default the state to ignored and only
  set it to processed if called. Or maybe we can only state the
  transforms that touched it and not worry at all about
  ignored/skipped.
- we can only tell if an element was processed on a leaf transform,
  not on a chain.
- we should add the transform's GUID to the report if they are
  enabled.
- because the transforms are in order, we can see who was the last
  transform that saw a given model element.

Tasks:

- add injector id property to asset elements. Populate this property
  during transforms. Actually we probably should just call it "source
  element id" and use the same name in the extraction model.
- add reporting elements to tracing for the modeling report.

Conclusions:

- the general conclusion, after some work in modeling the data types
  required for this, is that this is a subset of the use cases of the
  relational model. It will be yet another special case for reporting,
  which will answer some questions but not all. And in the future we
  will have to create yet another set of reports to answer different
  kinds of questions. The relational model is a more general solution
  to the problem. If we need to extend it we can write stored
  procedures in postgres.

*** COMPLETED Move visual studio related options to top profile       :story:
    CLOSED: [2019-10-26 Sat 17:38]
    :LOGBOOK:
    CLOCK: [2019-10-26 Sat 17:21]--[2019-10-26 Sat 17:38] =>  0:17
    :END:

At present we are duplicating all of the visual studio related options
across a number of models. We should use a profile instead.

*** COMPLETED Fix incorrect message keyword when detecting ODB includes :story:
    CLOSED: [2019-10-28 Mon 08:47]
    :LOGBOOK:
    CLOCK: [2019-10-28 Mon 08:41]--[2019-10-28 Mon 08:47] =>  0:06
    CLOCK: [2019-10-26 Sat 18:01]--[2019-10-26 Sat 18:24] =>  0:23
    :END:

We are using the non-existent keyword =FATAL= in the ODB portion of
the CMakeLists. We need update it to =FATAL_ERROR= as per the
documentation.

Actually the right solution for this is to remove this check
altogether. We don't really know how the user is finding ODB and the
more checks we do the larger the interface between our generated cmake
file and the regular cmake files. By removing the check we pass the
work to the user.

Links:

- [[https://cmake.org/cmake/help/v3.0/command/message.html][message]]

*** COMPLETED Meta-data keys are in the inverse order                 :story:
    CLOSED: [2019-11-04 Mon 22:14]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 22:11]--[2019-11-04 Mon 22:14] =>  0:03
    CLOCK: [2019-11-04 Mon 21:35]--[2019-11-04 Mon 22:11] =>  0:36
    :END:

Whilst investigation an issue with cycles, we noticed that all lists
within meta-data appear to be in inverse order. Fix this and
regenerate all models accordingly.

*** COMPLETED Creating reference cycles produces strange errors       :story:
    CLOSED: [2019-11-04 Mon 22:51]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 22:52]--[2019-11-04 Mon 23:00] =>  0:08
    CLOCK: [2019-11-04 Mon 22:14]--[2019-11-04 Mon 22:51] =>  0:37
    CLOCK: [2019-11-04 Mon 20:50]--[2019-11-04 Mon 21:34] =>  0:44
    CLOCK: [2019-11-04 Mon 20:01]--[2019-11-04 Mon 20:09] =>  0:08
    CLOCK: [2019-11-04 Mon 17:46]--[2019-11-04 Mon 18:30] =>  0:44
    CLOCK: [2019-11-04 Mon 08:00]--[2019-11-04 Mon 08:40] =>  0:40
    CLOCK: [2019-11-01 Fri 16:36]--[2019-11-01 Fri 16:52] =>  0:16
    CLOCK: [2019-11-01 Fri 14:20]--[2019-11-01 Fri 16:35] =>  2:15
    :END:

If a model A references another model B and model B also references
model A, dogen does not detect the cycle. This results in the not very
obvious error of having duplicate types:

: std::exception::what: More than one master segment found. Last: dogen.variability.registrar

What we should do instead is to detect the cycle when loading the
models and provide a sensible error message to the user.

Notes:

- add a data structure in the injection model set to capture reference
  information.
- add a validator as part of the IMS chain to validate that there are
  no cycles.
- add a stack to the validator to provide context when cycles occur.
- create a tracing report that takes in the data structures.

Problems:

- references seem to have been processed in reverse order.

*** COMPLETED Error on duplicate references                           :story:
    CLOSED: [2019-11-04 Mon 23:21]
    :LOGBOOK:
    CLOCK: [2019-11-04 Mon 23:25]--[2019-11-04 Mon 23:32] =>  0:07
    CLOCK: [2019-11-04 Mon 23:16]--[2019-11-04 Mon 23:21] =>  0:05
    CLOCK: [2019-11-04 Mon 23:01]--[2019-11-04 Mon 23:15] =>  0:14
    :END:

We need to check to see what happens if you enter the same reference
multiple times. We should error.

We should also detect references to "self".

*** COMPLETED Generate model dependency graph                         :story:
    CLOSED: [2019-11-05 Tue 18:17]
    :LOGBOOK:
    CLOCK: [2019-11-05 Tue 18:17]--[2019-11-05 Tue 18:30] =>  0:13
    CLOCK: [2019-11-05 Tue 17:41]--[2019-11-05 Tue 18:16] =>  0:35
    CLOCK: [2019-11-05 Tue 08:32]--[2019-11-05 Tue 08:45] =>  0:13
    CLOCK: [2019-11-05 Tue 08:00]--[2019-11-05 Tue 08:31] =>  0:31
    CLOCK: [2019-11-04 Mon 23:40]--[2019-11-04 Mon 23:54] =>  0:14
    CLOCK: [2019-11-04 Mon 23:33]--[2019-11-04 Mon 23:39] =>  0:06
    CLOCK: [2019-11-04 Mon 23:22]--[2019-11-04 Mon 23:25] =>  0:03
    :END:

It would be nice to generate a tracing of the model dependencies. This
may not necessarily be part of tracing.

*** COMPLETED Make the building of the relational model conditional   :story:
    CLOSED: [2019-11-05 Tue 22:24]
    :LOGBOOK:
    CLOCK: [2019-11-05 Tue 22:18]--[2019-11-05 Tue 22:24] =>  0:06
    CLOCK: [2019-11-05 Tue 22:01]--[2019-11-05 Tue 22:17] =>  0:16
    CLOCK: [2019-11-05 Tue 21:59]--[2019-11-05 Tue 22:00] =>  0:01
    :END:

We should only build the relational model if ODB support is
present. Otherwise we should ignore this model. Dogen should still
function, but all code related to the relational model should be
excluded. This includes the command line options related to database
configuration.

We should also tell the users that dogen was built without relational
support.

*** COMPLETED Make all transform contexts code-generated              :story:
    CLOSED: [2019-11-06 Wed 18:15]
    :LOGBOOK:
    CLOCK: [2019-11-06 Wed 18:07]--[2019-11-06 Wed 18:15] =>  0:08
    :END:

Try generating the engine context, it seems there is no obvious reason
for it not to work.

*** COMPLETED Tracer numbering of dumped models is incorrect          :story:
    CLOSED: [2019-11-08 Fri 11:09]
    :LOGBOOK:
    CLOCK: [2019-11-08 Fri 10:43]--[2019-11-08 Fri 11:09] =>  0:26
    :END:

We seem to be skipping numbers when dumping trace models:

: 000-injection.dia.decoding_transform-71058be5-3e36-4ca7-9e7b-10cee985a07d-input.json
: 002-injection.dia.decoding_transform-71058be5-3e36-4ca7-9e7b-10cee985a07d-output.json
: 002-injector.transforms.tagged_values_overrides_transform-d92cdc73-e5b3-4e15-9559-b430ab40040f-input.json
: 004-injector.transforms.configuration_transform-57a397a6-843a-4b4b-943c-aa66a31bd34a-input.json
: 004-injector.transforms.tagged_values_overrides_transform-d92cdc73-e5b3-4e15-9559-b430ab40040f-output.json
: 006-injection.transforms.input_technical_space_transform-dea341a4-9406-4213-b252-d88fecf2f1a2-input.json
: 006-injector.transforms.configuration_transform-57a397a6-843a-4b4b-943c-aa66a31bd34a-output.json
: 008-injection.transforms.input_technical_space_transform-dea341a4-9406-4213-b252-d88fecf2f1a2-output.json
: 008-injection.transforms.references_transform-eaf2422c-9dd2-4da0-aaf0-908688e2721d-input.json
: 010-injection.transforms.references_transform-eaf2422c-9dd2-4da0-aaf0-908688e2721d-output.json
: 011-injection.transforms.model_production_chain-2560043d-867e-43be-bd2d-2fec62d05bcc-output.json

We have rejiged the tracing numbering and it now seems ok. Instead of
trying to have the same entry number for the input and output of a
transform, we now give them distinct numbers. This makes the logic
easier to follow.

*** COMPLETED Fix compilation errors in c++ impl nightly              :story:
    CLOSED: [2019-11-08 Fri 11:14]
    :LOGBOOK:
    CLOCK: [2019-07-14 Sun 14:03]--[2019-07-14 Sun 14:27] =>  0:24
    :END:

Ever since we moved to the new PC, we are now getting weird
compilation errors:

: ../../../../projects/cpp_ref_impl.cpp_98/tests/an_enumeration_tests.cpp:100:58: error: the result of the conversion is unspecified because ‘13’ is outside the range of type ‘cpp_ref_impl::cpp_98::an_enumeration’ [-Werror=conversion]

The problem appears to be that our push for the warning is no longer working:

: BOOST_AUTO_TEST_CASE(casting_invalid_enumeration_throws) {
: #if BOOST_COMP_GNUC
: #pragma GCC diagnostic push
: #pragma GCC diagnostic ignored "-Wconversion"
: #endif
:    using cpp_ref_impl::cpp_98::an_enumeration;
:   const an_enumeration r(static_cast<an_enumeration>(13));
: #if BOOST_COMP_GNUC
: #pragma GCC diagnostic pop
: #endif

This may be related to our use of boost macros without including
=predef.h=.

This problem seems to have gone away by itself.

*** COMPLETED Add support for meta-data overrides in Dogen            :story:
    CLOSED: [2019-11-08 Fri 15:56]
    :LOGBOOK:
    CLOCK: [2019-11-08 Fri 15:28]--[2019-11-08 Fri 15:39] =>  0:11
    CLOCK: [2019-11-08 Fri 15:01]--[2019-11-08 Fri 15:28] =>  0:27
    CLOCK: [2019-11-08 Fri 11:33]--[2019-11-08 Fri 12:21] =>  0:48
    CLOCK: [2019-11-08 Fri 11:27]--[2019-11-08 Fri 11:32] =>  0:05
    CLOCK: [2019-11-07 Thu 23:15]--[2019-11-08 Fri 00:02] =>  0:47
    CLOCK: [2019-11-07 Thu 22:35]--[2019-11-07 Thu 23:15] =>  0:40
    CLOCK: [2019-11-06 Wed 23:46]--[2019-11-06 Wed 23:58] =>  0:12
    CLOCK: [2019-11-06 Wed 23:42]--[2019-11-06 Wed 23:45] =>  0:03
    CLOCK: [2019-11-06 Wed 23:15]--[2019-11-06 Wed 23:41] =>  0:26
    CLOCK: [2019-11-06 Wed 22:58]--[2019-11-06 Wed 23:14] =>  0:16
    CLOCK: [2019-11-06 Wed 22:41]--[2019-11-06 Wed 22:57] =>  0:16
    CLOCK: [2019-11-06 Wed 18:20]--[2019-11-06 Wed 18:40] =>  0:20
    CLOCK: [2019-11-06 Wed 18:16]--[2019-11-06 Wed 18:19] =>  0:03
    CLOCK: [2019-11-06 Wed 17:49]--[2019-11-06 Wed 18:06] =>  0:17
    CLOCK: [2019-11-06 Wed 08:07]--[2019-11-06 Wed 08:32] =>  0:19
    :END:

In order to support the scenario of builds with generated code we need
to be able to override the profile of a model. To do so we should
build a generic mechanism to override meta-data in a model.

Notes:

- see [[*Consider creating a test build for all facets][Consider creating a test build for all facets]]
- create a new command line flag: =variabulity-override=
- it takes a triplet in the form of

: MODEL_NAME,ELEMENT_NAME,ATTRIBUTE_NAME,KEY,VALUE

- we parse these tuples into a container and then use it in the
  variability transforms.
- we need a data structure that reflects the topology: global,
  element, property. It must be keyed by model name, element name,
  attribute name.
- the command line option is parsed and expanded into this new data
  structure. The data structure is kept in injection context. Just
  before calling the configuration factory, we need to locate the
  appropriate overrides. We supply these to the configuration factory.
- the factory takes the appropriate decision:
  - for scalar value types we merely override the value.
  - for collections we push back.
- we should also mark unused overrides and throw if there are any. We
  should record the original override string.
- the remainder of variability processing will work as at present. We
  just must ensure we override prior to any profile merging/expansion.

*Previous understanding*

In the past we had enabled a lot of facets on the dogen models to
serve as part of the testing infrastructure. However, its no longer
feasible to do this because the build is taking too long. This is not
ideal as the reference models just can't capture all of the complexity
of a codebase like dogen's so we lost some testability with this
move. What would be really nice is if we could create "test builds":

- given a set of test models, copy them somewhere, generate a product
  configuration with some kind of override that enables all facets
  everywhere. some will just not come through like ORM.
- build the product. all handcrafted code is now blank but all facets
  are coming though.
- this could be part of the ctest script, as a "mode" - product
  generation test. Every time there is a commit to a product the build
  kicks in.

Notes:

- one way to achieve this would be to force the profile of the
  model. However, we are moving away from profiles, and in the future
  there will be a list of stereotypes associated with the model. Then
  it will be much harder to figure out what stereotypes do what and to
  overwrite them.
- an alternative would be to have some kind of "test mode"; when
  handling enablement, we'd check the "mode". If we're in test mode,
  we simply enable all and ignore any other settings. We could have a
  "force enable" flag or some such like we do for
  overwriting. However, we may then hit another problem: enabling all
  facets may result in non-buildable models:
  - facets may be incompatible. This is not a problem at present.
  - handcrafted classes may result in code that does not
    compile. Shouldn't though because we are still checking the status
    of the attributes.

Conclusions:

- create a new flag: =force-enablement=. When set to true, we ignore
  all enablement settings and generate all facets. We do not generate
  all kernels though (e.g. the kernel must be on in the model).
- create a script that copies the models to a new product and
  generates them with fore-enablement. This will only work when we can
  generate products.
- as facets are enabled, tests are automatically generated for them.
- build the result and run all tests.

Merged stories:

*New approach to model testing*                                    :story:*

In the beginning we generated all models with all facets, even the
dogen core models. The idea was to test the generator even though
these facets were not useful for the product. This was really useful
because the dogen models are much more realistic than the test models
and due to this we picked up a number of bugs. However, we have now
hit the maximum build times on travis and we need to start removing
all ballast. This will mean we lose these valuable tests. The
alternative is to create these tests on the fly:

- create a new override flag that forces all facets to be emitted.
- create a new test facet with templates that are dependent on the
  enabled facets; each test tests the dependent facet.
- create a ctest nightly build that generates code using these new
  facets, compiles it and runs all tests.
- we need some meta-data to "ignore" some modeling elements for
  certain facets such as composition which are known to be broken. Or
  maybe we should just leave the tests as red so we know.
- the tests should be designed not to use templates etc to make the
  debug dumps really obvious (unlike the existing tests). It may even
  make more sense to test each type individually so that when the test
  fails its really obvious:

: MY_TYPE_serialisation_roundtrips_correctly

  this way when we look at CDash we know exactly which types failed to
  serialise.

During the transition phase, we will remove all of the existing tests.

*** COMPLETED Some model names are too long for windows               :story:
    CLOSED: [2019-11-15 Fri 11:15]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 10:47]--[2019-11-15 Fri 11:15] =>  0:28
    :END:

We are hitting a familiar error on windows builds:

: [00:15:17] C:\Program Files (x86)\Microsoft Visual
: Studio\2017\Community\Common7\IDE\VC\VCTargets\Microsoft.CppBuild.targets(321,5):
: error MSB3491: Could not write lines to file
: "cpp_ref_impl.do_not_delete_empty_dirs.generated_tests.dir\Release\cpp_ref_.590618F1.tlog\cpp_ref_impl.do_not_delete_empty_dirs.generated_tests.lastbuildstate". The
: specified path, file name, or both are too long. The fully qualified
: file name must be less than 260 characters, and the directory name
: must be less than 248 characters.
: [C:\projects\cpp-ref-impl\build\output\msvc\Release\projects\cpp_ref_impl.do_not_delete_empty_dirs\generated_tests\cpp_ref_impl.do_not_delete_empty_dirs.generated_tests.vcxproj]

We need to rename the following models:

- two layers with objects: two layers.
- do not delete empty folders: skip empty folders

*** COMPLETED Fake type in injection.json model                       :story:
    CLOSED: [2019-11-15 Fri 11:24]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 11:32]--[2019-11-15 Fri 11:41] =>  0:09
    CLOCK: [2019-11-15 Fri 11:15]--[2019-11-15 Fri 11:24] =>  0:09
    :END:

We added a fake type to the injection.json model (see [[*Generated tests fail when model has nothing to test][Generated tests
fail when model has nothing to test]]). There is a better solution to
this: just disable generated tests in this model for now.

*** COMPLETED Add conditional logic for the inclusion of generated tests :story:
    CLOSED: [2019-11-15 Fri 11:33]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 11:25]--[2019-11-15 Fri 11:32] =>  0:07
    :END:

At present the generated CMake files do not take into account the
generated test folder:

: if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
:    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
: endif()

Actually the right solution is to remove this hand crafted code from
generated files. The generated code will insert the =add_subdirectory=
if the facet is enabled as required.

*** COMPLETED Split generated tests from manual tests                 :story:
    CLOSED: [2019-11-15 Fri 11:56]
    :LOGBOOK:
    CLOCK: [2019-11-14 Thu 08:13]--[2019-11-14 Thu 08:57] =>  0:44
    CLOCK: [2019-11-13 Wed 21:33]--[2019-11-13 Wed 22:52] =>  1:19
    CLOCK: [2019-11-13 Wed 21:31]--[2019-11-13 Wed 21:32] =>  0:01
    CLOCK: [2019-11-13 Wed 21:21]--[2019-11-13 Wed 21:31] =>  0:10
    CLOCK: [2019-11-13 Wed 18:24]--[2019-11-13 Wed 18:41] =>  0:17
    CLOCK: [2019-11-13 Wed 18:10]--[2019-11-13 Wed 18:23] =>  0:13
    CLOCK: [2019-11-13 Wed 17:44]--[2019-11-13 Wed 18:09] =>  0:25
    CLOCK: [2019-11-13 Wed 08:22]--[2019-11-13 Wed 08:36] =>  0:14
    CLOCK: [2019-11-13 Wed 08:00]--[2019-11-13 Wed 08:17] =>  0:17
    CLOCK: [2019-11-12 Tue 21:23]--[2019-11-12 Tue 22:39] =>  1:16
    CLOCK: [2019-11-12 Tue 19:25]--[2019-11-12 Tue 19:55] =>  0:30
    CLOCK: [2019-11-08 Fri 16:35]--[2019-11-08 Fri 17:00] =>  0:25
    :END:

We need to isolate the generated tests into its own binary, to allows
to run them in isolation from manual tests.

Notes:

- rename the facet to something like =generated_tests=, splitting out
  the manual tests from it.
- add CMake options to enable the different types of tests:
  - =WITH_MANUAL_TESTS=
  - =WITH_GENERATED_TESTS=
- create different top-level targets:
  - run all manual tests
  - run all generated tests
- make run all tests depend on both targets. With this it should now
  be possible to run only the generated or the manual tests.
- nightly:
  - create two clones of dogen, one just for the purposes of running
    the tests. Set the env var to point to this clone. Give it a
    meaningful name (e.g. "clean_dogen"?)
  - at present we only have =ctest_build=. However, we need a two step
    build process. First we enable overrides, then we run =gad= then
    we build all.
  - the right solution is to have a separate process that checks out
    dogen from git, builds CLI, rungs =gad= with overrides and then
    commits and pushes the result. The problem though is that we need
    to always start from master, and disregard any previous state. For
    this we have several possibilities:
    - create a new branch every day, using the same name. The downside
      is that we cannot easily see previous state. However, it should
      always be reproducible because we started from a known
      commit. On the plus side, branch management is easy. We just
      need to make sure appveyor and travis are ignoring this branch.
    - create a new branch per day. This way we can easily compare
      today with yesterday for example. However, we now need a way to
      manage the branches - i.e. if we reach day 7 go back to day 1,
      etc.
    - find a way to do this using a single branch. Maybe there is a
      way in git to "merge" branches in a way that we always start
      with the current commit in master. However, it must be based on
      some kind of "force" approach because we can't merge or rebase
      (we do not want to carry state from previous commits). We need
      to investigate what can be done within git. For example, "undo"
      previous commit if commit has string XYZ, merge master. Note: we
      tried implementing this approach, but there are a lot of
      possible states the branches can be in, and its very difficult
      to create a script that copes with all of the permutations.
    - do not commit the state. Simply run a script that pulls latest,
      builds CLI and calls gad with overrides. Then do a regular
      nightly. The git pull will do nothing (so we can't see what
      commits we have in the nightly).
- check in the script for nightly builds.

*Previous understanding*

When we created the tests facet, we did not exactly follow the
existing framework. Normally we have the concept of a project
(e.g. say "directory settings"), which contains what is now understood
as "destinations". In C++ these can be:

- include
- source (=src= folder)

Destinations are just folders in the filesystem. These can be mapped
to any name defined by the user. Normally, we then place facets inside
of destinations. For example, the facet "types" can be projected into
the include and source destinations:

- =include/types=
- =src/types=

Some facets do not have a facet folder, and as such are projected
directly onto the project directory (CMakeLists, etc).

 When we added tests, we created a destination called =tests= but we
 also created a facet called =tests=. If all had gone according to the
 existing logic, we would have ended up with:

- =tests/tests=

However, we glossed over all of the above and when setting the file
paths we used the facet directory, but when setting other things (such
as say the CMakeLists directories), we used the destination. The net
result is that, for all intents and purposes, it looked as if we were
following the above setup, but somehow had ended up with:

- =tests=

When we recently updated the directory settings model, it all came
undone. As a hack, we did:

: #DOGEN masd.generation.cpp.tests.directory=tests_dir
: #DOGEN masd.generation.cpp.tests_directory_name=tests_dir

To maintain the existing logic, but in truth we need a proper fix. The
right solution is to give a name to the facet which is not =tests=. It
should really be something evocative of its functionality:

- generated tests? but then we don't partition generated code anywhere
  else. It would be useful though, so that we can easily locate
  handcrafted tests - and even exclude them too. If we also had
  separate CMakeFiles we could have a top-level variable that excludes
  generated tests for compilation.
- tests for code generated code?
- structural tests?

Basically we need a fitting name for this facet. Once found, we need
to move all tests into this directory.

Note that we cannot do the locator clean up until we sort out this mess.

*** COMPLETED Use clang9 and GCC 9 in nightly and CI                  :story:
    CLOSED: [2019-11-15 Fri 12:09]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 11:54]--[2019-11-15 Fri 11:55] =>  0:01
    :END:

We seem to still be in clang 8 and gcc 8 in some places. Update the
compilers.

*** COMPLETED Full generation produces invalid code                   :story:
    CLOSED: [2019-11-15 Fri 16:26]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 15:48]--[2019-11-15 Fri 16:26] =>  0:38
    :END:

When using full generation to build the utility model, we generate
invalid forward declarations for serialisation - they are missing the
includes. We need to understand what causes this and fix it.

The problem was that we disabled forward declarations on this
type. Enabling it fixes the issue.

*** COMPLETED Code coverage has not been updated since September      :story:
    CLOSED: [2019-11-15 Fri 16:40]
    :LOGBOOK:
    CLOCK: [2019-11-15 Fri 16:27]--[2019-11-15 Fri 16:42] =>  0:15
    CLOCK: [2019-11-15 Fri 15:10]--[2019-11-15 Fri 15:41] =>  0:31
    CLOCK: [2019-11-15 Fri 15:03]--[2019-11-15 Fri 15:09] =>  0:06
    CLOCK: [2019-11-15 Fri 14:35]--[2019-11-15 Fri 15:02] =>  0:27
    :END:

For some reason we have not been generating coverage reports for a
couple of months. In addition, don't submit CI to both coveralls and
codecov. We want CI to go to coveralls and nightlies to go to codecov.

*** COMPLETED Time spent fixing emacs issues                          :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-11-18 Mon 08:13]--[2019-11-18 Mon 08:23] =>  0:10
    CLOCK: [2019-11-08 Fri 10:18]--[2019-11-08 Fri 10:43] =>  0:25
    CLOCK: [2019-11-07 Thu 21:15]--[2019-11-07 Thu 22:34] =>  1:19
    :END:

- fix issue with smartparens: For some random reason emacs on our
  desktop PC is behaving very strangely. It seems that the problems
  stem from smartparens. Try to disable this in prelude.
- fix issue with treemacs: we are getting a lot of weird errors in
  treemacs since the last update, which happened because we were
  trying to fix smartparens. See issue: [[https://github.com/Alexander-Miller/treemacs/issues/562][When tag follow mode is
  enabled, the message buffer is flooded with: {Treemacs} Encountered
  error while following tag at point: (wrong-type-argument listp
  DimCounter) #562]]

*** COMPLETED Consider creating a test build for all facets           :story:
    CLOSED: [2019-11-18 Mon 08:25]
    :LOGBOOK:
    CLOCK: [2019-11-16 Sat 16:22]--[2019-11-16 Sat 16:24] =>  0:02
    CLOCK: [2019-11-16 Sat 16:16]--[2019-11-16 Sat 16:21] =>  0:05
    CLOCK: [2019-11-16 Sat 16:01]--[2019-11-16 Sat 16:15] =>  0:14
    CLOCK: [2019-11-15 Fri 18:00]--[2019-11-15 Fri 18:04] =>  0:04
    CLOCK: [2019-11-15 Fri 17:56]--[2019-11-15 Fri 17:59] =>  0:03
    CLOCK: [2019-11-15 Fri 16:42]--[2019-11-15 Fri 17:15] =>  0:33
    CLOCK: [2019-11-15 Fri 12:10]--[2019-11-15 Fri 12:38] =>  0:28
    CLOCK: [2019-11-15 Fri 11:56]--[2019-11-15 Fri 12:09] =>  0:13
    CLOCK: [2019-11-15 Fri 11:42]--[2019-11-15 Fri 11:53] =>  0:11
    :END:

We can't afford to generate test code in Dogen for the continuous
builds because we don't have enough build time to compile all of the
generated code. This is true even when we are just generating a few
facets (=test_data=, =types=, =tests=). However we definitely want to
test the generated code in real models.

The solution for this is to allow "variability overrides" (see [[*Add support for meta-data overrides in Dogen][Add
support for meta-data overrides in Dogen]]). Once we have this
functionality in place, we can then update our nightly builds.

Notes:

- create a test profile that enables all facets.
- in CMake, add an override to this profile for nightlies. This could
  be a parameter passed in from CTest.
- run the =gad= target in CTest first, then build then run all tests.
- because the nightly is running under our control, we can easily
  check CDash for errors and look at the generated source to
  investigate the problem.
- we should setup nightlies for Windows and OSX as well.
- remove all of the test facets from the main repo (e.g. =test_data=,
  =tests=).
- note that this approach will also resolve the problem with ignoring
  tests because we don't need to have them in the version control
  system any longer. We should remove all of the regexes ignoring
  tests as part of this work.
- this approach could be extended to conversion: once we fix all of
  the issues with JSON conversion, we don't need to have the JSON
  models in version control. We can generate them on the fly for
  nightlies only. It will require a bit of thinking because the tests
  are hard coded.
- the key thing though is the overall build time must be below the
  threshold. Maybe we can have this on a nightly, running on our own
  hardware.
- manually created generation tests at present do not use the
  overrides; this means that we will now have a lot of spurious
  differences in code generation due to this. As a result we must not
  run the generation tests whenever we run the generated tests. Which
  means we must somehow split these two kinds of tests and make them
  mutually exclusive. This is best achieved by having two different
  nightly builds:
  - "manual" build: as is at present minus all the generated tests. We
    do not need to ignore generated tests because there will be none
    in git.
  - "generated" build: only runs the generated tests. Must not run the
    manully generated tests.

*** POSTPONED Move registrar into assets                              :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-10-24 Thu 08:20]--[2019-10-24 Thu 08:43] =>  0:23
    CLOCK: [2019-06-12 Wed 15:08]--[2019-06-12 Wed 17:09] =>  2:01
    CLOCK: [2019-06-11 Tue 21:31]--[2019-06-11 Tue 22:52] =>  1:21
    CLOCK: [2019-06-11 Tue 11:57]--[2019-06-11 Tue 12:20] =>  0:23
    CLOCK: [2019-06-11 Tue 11:52]--[2019-06-11 Tue 11:56] =>  0:04
    CLOCK: [2019-06-11 Tue 11:16]--[2019-06-11 Tue 11:51] =>  0:35
    CLOCK: [2019-06-03 Mon 19:34]--[2019-06-03 Mon 19:45] =>  0:11
    CLOCK: [2019-06-03 Mon 19:27]--[2019-06-03 Mon 19:34] =>  0:07
    CLOCK: [2019-06-03 Mon 19:02]--[2019-06-03 Mon 19:26] =>  0:24
    :END:

Move the registrar type into assets, in the quickest way possible.

Notes:

- In order to avoid blocking due to lots of analysis, we need
  to split this story into three:
  - first, we need to just move the registrar as is into assets.
  - a second story is to clean up the existing registrar code to have
    less templates and possibly address the existing registration
    bugs. We could also look into calling the registrars for
    referenced models automatically as part of this work (at present
    we are doing this manually).
  - finally, we need some meta-level refactoring to figure out if the
    pattern can be generalised to include initialisers, etc.
  In general that should be our approach: try to split out the
  capturing of patterns into as many steps as possible, to make sure
  we don't get overwhelmed as we implement things.
- we need to keep track of all type registrars on referenced models,
  not on the referenced models themselves. We need to know which
  models we referenced directly, and then find the registrars for
  those models.
- leaves need to know of the registrar. This is so that we can call it
  in their generated tests. We could use the registrar transform to go
  and find all leaves and populate their registrar name.
- current state is that we cannot generate the registrar for some
  reason.
- test model with registrar is C++ model. Type is called
  registrar. Its probably not a good idea to also call it registrar -
  wouldn't that clash with the existing type?
- we should have a warning/error: if using boost serialisation with a
  model that has inheritance, the registrar should be present. Added
  to warnings story.

*** POSTPONED Upgrade to boost 1.70                                   :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-09-10 Tue 13:05]--[2019-09-10 Tue 16:53] =>  3:48
    CLOCK: [2019-09-10 Tue 10:15]--[2019-09-10 Tue 12:39] =>  2:24
    CLOCK: [2019-09-10 Tue 08:40]--[2019-09-10 Tue 10:14] =>  1:34
    CLOCK: [2019-09-05 Thu 11:07]--[2019-09-05 Thu 11:17] =>  0:10
    CLOCK: [2019-07-14 Sun 14:34]--[2019-07-14 Sun 16:33] =>  1:59
    CLOCK: [2019-07-14 Sun 14:28]--[2019-07-14 Sun 14:33] =>  0:05
    :END:

We should try to upgrade to latest boost.

Notes:

- the problem appears to be that with OSX we do not have a compiler
  installed that can compile vcpkg. It is not clear how we did it
  before. The installed XCode compiler is too old and we do not have
  homebrew for gcc.
- installed LLVM 7. Ninja then went on a strange loop, regenerating
  CMake files. This was because NTP had not been working on OSX for
  some reason, and the clock was in the past.
- compiling with clang 7 causes the =-lc++fs= linking error. Tried
  compiling with clang 8.
- Compilation required setting LDFLAGS -L to point to the lib
  directory of the download, else the static library for filesystem
  could not be location.
- We may have linking problems now that we are using XCode 10 in
  travis and clang 8 to build vcpkg dependencies.
- ODB 2.5 no longer works due to a git ref mismatch. Not clear why
  that would be but the object we were referencing no longer exists in
  code synthesis git repo.
- the ref for ODB SQL lite 2.5.0-b.9 does not seem to exist in their
  repo any longer. Due to this, the OSX build is failing. For now we
  shall try to update excluding that dependency, given we are not even
  using it.
- boost regex fails to build. The problem is that we are picking up
  the system compiler instead of CXX. It is not clear why that
  is. Maybe we got lucky in the past because we were using c++14 but
  now with c++17 system clang fails to compile because it does not
  have c++ 17 support.
- nightlies are now failing with a missing reference to SQL lite.

*** POSTPONED Setup laptop to work on dogen                           :story:
    CLOSED: [2019-11-18 Mon 08:24]
    :LOGBOOK:
    CLOCK: [2019-10-28 Mon 08:25]--[2019-10-28 Mon 08:41] =>  0:16
    CLOCK: [2019-10-28 Mon 08:19]--[2019-10-28 Mon 08:25] =>  0:06
    CLOCK: [2019-10-28 Mon 08:13]--[2019-10-28 Mon 08:19] =>  0:06
    CLOCK: [2019-10-24 Thu 08:03]--[2019-10-24 Thu 08:20] =>  0:17
    :END:

We haven't used the laptop for dogen for quite a bit so its behind the
main machine. Get it in a shape to do development again.

Items missing:

- consolas font. done.
- dir locals for projects
- polymode
- build2
- odb

*** POSTPONED Org-mode as a carrier format for modeling               :story:
    CLOSED: [2019-11-18 Mon 08:25]
    :LOGBOOK:
    CLOCK: [2019-06-05 Wed 14:17]--[2019-06-05 Wed 18:02] =>  3:45
    CLOCK: [2019-06-05 Wed 12:17]--[2019-06-05 Wed 12:42] =>  0:25
    CLOCK: [2019-06-05 Wed 10:50]--[2019-06-05 Wed 12:17] =>  1:27
    :END:

This is a bit of a weird idea, but may just work; this story is a
placeholder to capture ideas in this space. Consider a org-mode
file as a model. Ideas:

- the top-level properties are all model properties. For example, if
  you add text at the top, that is a model comment.
- we can also make use of the exact same format for Dogen comments as
  we do in Dia, with =#DOGEN= markers.
- stereotypes and other meta-data can be conveyed using org-mode
  properties. In addition, due to org-babel, we can include code
  snippets on any programming language, with some (minimal) IDE-like
  integration.
- we could also include the GUIDs for merging as org-mode properties.
- once we create a C++ stand-alone product to represent org-mode
  documents, we can just create an adapter for it as an injector.
- there already is some support for creating state-machines in
  org-mode: [[https://orgmode.org/worg/org-tutorials/org-dot-diagrams.html][Org tutorial on generating simple process diagrams using
  dot and tables]]

Links:

- [[https://github.com/mirkoboehm/OrgModeParser][OrgModeParser]]: requires QT.
- [[https://www.reddit.com/r/emacs/comments/bciwiz/does_orgmode_have_a_formal_grammar_or_some_subset/][Does orgmode have a formal grammar, or some subset of it?]]
- [[https://orgmode.org/worg/dev/org-syntax.html][Org Syntax (draft)]]
- [[https://orgmode.org/worg/dev/org-element-api.html][Org Element API]]
- [[https://github.com/ngortheone/org-rs][org-rs]]: rust library for org-mode.
- [[https://github.com/felipealmeida/orgmode-parsers][orgmode-parsers]]

*** POSTPONED Add relational tracing support                          :story:
    CLOSED: [2019-11-18 Mon 08:25]
    :LOGBOOK:
    CLOCK: [2019-11-16 Sat 17:22]--[2019-11-16 Sat 19:13] =>  1:51
    CLOCK: [2019-11-16 Sat 16:25]--[2019-11-16 Sat 17:21] =>  0:56
    CLOCK: [2019-11-07 Thu 20:11]--[2019-11-07 Thu 20:40] =>  0:29
    CLOCK: [2019-11-07 Thu 19:55]--[2019-11-07 Thu 20:10] =>  0:15
    CLOCK: [2019-11-07 Thu 18:31]--[2019-11-07 Thu 18:33] =>  0:02
    CLOCK: [2019-11-07 Thu 17:41]--[2019-11-07 Thu 18:30] =>  0:49
    CLOCK: [2019-11-07 Thu 08:31]--[2019-11-07 Thu 08:55] =>  0:24
    CLOCK: [2019-11-07 Thu 08:14]--[2019-11-07 Thu 08:19] =>  0:05
    CLOCK: [2019-11-01 Fri 11:18]--[2019-11-01 Fri 11:54] =>  0:36
    CLOCK: [2019-10-29 Tue 18:07]--[2019-10-29 Tue 18:30] =>  0:23
    CLOCK: [2019-10-29 Tue 17:46]--[2019-10-29 Tue 18:06] =>  0:20
    CLOCK: [2019-10-29 Tue 08:55]--[2019-10-29 Tue 09:03] =>  0:08
    CLOCK: [2019-10-29 Tue 08:38]--[2019-10-29 Tue 08:48] =>  0:10
    CLOCK: [2019-10-28 Mon 18:55]--[2019-10-28 Mon 19:10] =>  0:15
    CLOCK: [2019-10-28 Mon 17:45]--[2019-10-28 Mon 18:20] =>  0:35
    CLOCK: [2019-10-28 Mon 17:29]--[2019-10-28 Mon 17:39] =>  0:10
    CLOCK: [2019-10-28 Mon 08:48]--[2019-10-28 Mon 08:52] =>  0:04
    CLOCK: [2019-10-27 Sun 12:05]--[2019-10-27 Sun 12:40] =>  0:48
    CLOCK: [2019-10-27 Sun 08:57]--[2019-10-27 Sun 09:04] =>  0:07
    CLOCK: [2019-10-27 Sun 08:29]--[2019-10-27 Sun 08:56] =>  0:27
    CLOCK: [2019-10-26 Sat 17:54]--[2019-10-26 Sat 18:00] =>  0:06
    CLOCK: [2019-10-26 Sat 17:47]--[2019-10-26 Sat 17:53] =>  0:06
    CLOCK: [2019-10-26 Sat 17:40]--[2019-10-26 Sat 17:46] =>  0:06
    CLOCK: [2019-10-26 Sat 16:01]--[2019-10-26 Sat 16:41] =>  0:40
    :END:

Whenever we bump into a problem we seem to spend a lot of time going
through the log files and trace files trying to figure out where the
problem is happening. Have a quick go in trying to implement a
relational model for tracing to see if we can transfer the bulk of the
data into a relational format which we can query via SQL.

We've created a basic relational model for tracing. The relational
part of it seems straightforward (ish); the problem is the integration
of the tracer with the relational model. At present we rely on the
fact that all traceable objects have IO enabled; this works because
the code generator creates the IO facet, which is then used by the
write method in utility to convert any model type into a
string. However, we now need to change the approach: we need multiple
tracing backends:

- file tracer
- database tracer.

The file tracer is more or less the current tracer. The database
tracer needs to decompose the objects in existing models into a
relational representation. In an ideal world, the user would configure
the tracer to use one of the two backends and the remaining usage
would be transparent. However, we cannot have an interface for the
tracer backend that uses template methods because then we'd need
virtual template functions, it seems.

Another alternative is to make the tracer aware of the model objects
it is tracing. This is also not ideal because we would create cycles
in the design.

In effect we need to somehow implement a similar approach to the existing
tracer: rely on global template functions a-la =operator<<= to
decompose objects into their relational representations and then
supply those to the backend. It is not very clear how this would
work. For now we've postponed this approach as it seems its not going
to be a quick win.

We should approach this incrementally. Next time we have a bit of
spare time, we need to generate the model and then create the adapters
from existing models. Finally we can look at how it will be integrated
with tracing.

Notes:

- compilation generates an ODB error:

: FATALODB include directories not defined.

- the key difference between northwind and tracing is that we have a
  namespace. The application of the schema pragma is probably not
  working due to this. We need to look into the transform to see how
  that pragma propagates.
- the problem arises because we are only populating the primitive's
  properties if there is a top-level pragma. As the schema is not
  populated for the namespace, there isn't one. It is not clear why
  one would want to skip properties such as DB member if there isn't a
  schema, but perhaps this is due to some ODB error. We should
  probably issue an error or warning if we cannot generate code
  without a schema name.
- with regards to the relational model, the problem is that we can't
  really create a schema for each namespace in a model because schemas
  are not really like namespaces. The entities in a schema should
  really be self-contained and not refer to other schemas or else the
  database will be confusing to use. For example in postgres we will
  need to set the schema path, etc in order to see the different
  tables. One possible solution is to set the schema name to the same
  value for all namespaces (e.g. =dogen=). This would then allow us to
  have namespaces in C++ but not in the database.
- it seems foreign keys are not supported at present. We probably need
  support for this in order to query quickly or else we will have to
  manually setup indices for each of these joining fields.
- we need a command line option to choose the tracing backend
  (e.g. file or database). We also need the database configuration
  parameters: hostname, port, database, user.
- we need to refactor tracer as follows:
  - update the tracer interface to take actual types rather than
    templates.
  - create a top-level interface for the notion of a backend.
  - create two implementations of the backend: file and relational.
  - move all the file related code to the file backend.
  - implement adapters for each model to convert them into relational
    model types.
  - implement the relational backend.
- Add relational model to the dogen model tests.

Merged stories:

*Scripts for loading traces into postgres*

- rationale: this story is superseded by having a relational model.

It would be really nice if as part of the tracing generation we also
generated a set of SQL scripts that:

- created a number of tables
- copied all of the generated data into the database
- added a number of utility functions such as get elements in model, etc.

Over time we could build up functionality but to start off with we
just want something really simple that copies all of the
files. Interestingly this "looks" like a job for dogen. It would be
nice to have a meta-model element for this etc.

In the future it would be nice to have a think about the schema so
that we could do joins etc. For example:

- show me all transforms with element of type X (the state of the
  element at each transform).

We should also take into account multiple runs. Perhaps a more
adequate solution is to create a dogen library that has the ORM
support for this. Once we have proper JSON serialisation we can store
the objects as JSON serialisable, allowing us to re-run transforms,
etc.

Notes:

- ensure we upload the file name or at least the coordinates to the
  transform graph with the data so that we know what it refers to.
- rename relational database enum to just database
- rename hostname to just host

*Improved understanding*

Better than uploading a whole load of JSON blobs and then having to do
a number of really complex queries, is to have a ORM schema that is
designed to capture the data in the format we're interested in. Then
we could do very simple queries. What we really care about is
capturing all attributes of the model as it changes across the
transformations. We also care about the relationships between
transformations. We also need a way to uniquely identify elements
across their entire lifecycle. A simple way would be to create a hash
of the file name of the model, column and line number. We can then
associate other IDs to this one such as dia ID, etc.

We need to create a set of adaptors that convert an existing model
(injection, coding, etc) into the ORM model and then write the ORM
model into the database. The ORM model does not need as much detail
and structure as a regular model; for example, names can be flattened
or linked into IDs (e.g. name table), etc. Whatever makes sense from a
relational perspective.

It would also be nice to dump the log into the database so that we
could do simple correlations such as "what was logged between the
start and end of this transform?"

Interestingly, this would also allow us to compare things between
runs. The schema should be designed with this in mind.

** Deprecated
*** CANCELLED Reactivate injection.dia tests                          :story:
    CLOSED: [2019-06-03 Mon 20:01]

*Rationale*: these tests have now been removed when serialisation
support was removed.

We seem to have a number of tests commented out in
injection.dia. Investigate why and if possible, reactivate them.
*** CANCELLED Create a test for "not deleting empty directories"      :story:
    CLOSED: [2019-11-15 Fri 11:23]

*Rationale*: test existed but name of test did not match the model
name.

At present we have created a model to test that setting to false the
flag =masd.extraction.delete_empty_directories= results in not
deleting empty directories. We were probably testing this correctly
during the test model sanitizer days but when moving the tests into
dogen we seem to have lost this one.

The test should create one or more empty folders inside the model, run
code generation and ensure the folders are still there afterwards.
*** CANCELLED Serialisation forward declarations are cyclical         :story:
    CLOSED: [2019-11-15 Fri 16:02]

*Rationale*: we were looking into the main serialisation header, not
the forward declaration.

At present we are including the serialisation forward declaration from
the types header, and also including the types header from the
serialisation forward declaration. We should probably include the
types forward declaration.
