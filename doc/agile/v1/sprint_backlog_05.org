#+title: Sprint Backlog 05
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Sort out concepts and profiles.
- Resume and progress the work on moving "generic" types from the
  quilt models into yarn.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2017-09-18 Mon 00:04]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *59:18* |       |       | 100.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 59:18   |       |       | 100.0 |
| Active                                                                      |         | 59:18 |       | 100.0 |
| COMPLETED Edit release notes for previous sprint                            |         |       |  0:44 |   1.2 |
| COMPLETED Sprint and product backlog grooming                               |         |       |  1:43 |   2.9 |
| COMPLETED Remove object types in yarn                                       |         |       |  0:52 |   1.5 |
| COMPLETED Clean-up readme                                                   |         |       |  1:15 |   2.1 |
| COMPLETED Rename transformers to adapters                                   |         |       |  0:28 |   0.8 |
| COMPLETED Analysis on annotations and profiles                              |         |       |  2:26 |   4.1 |
| COMPLETED Use namespaced stereotypes                                        |         |       |  1:49 |   3.1 |
| COMPLETED Tailor dogen models                                               |         |       |  0:21 |   0.6 |
| COMPLETED Rename yarn concepts                                              |         |       |  7:49 |  13.2 |
| COMPLETED Remove extra copying in mapper                                    |         |       |  0:05 |   0.1 |
| COMPLETED Investigate the JSON break on Northwind model                     |         |       |  0:57 |   1.6 |
| COMPLETED Rename ODB parameters                                             |         |       |  0:11 |   0.3 |
| COMPLETED Investigate usage of origin type                                  |         |       |  0:15 |   0.4 |
| COMPLETED Improve dumping of models                                         |         |       | 12:59 |  21.9 |
| COMPLETED Create yarn options                                               |         |       |  1:06 |   1.9 |
| COMPLETED Merge knit with yarn                                              |         |       |  1:13 |   2.1 |
| COMPLETED Add probing support to code generation tests                      |         |       |  2:45 |   4.6 |
| COMPLETED Find some way of scoping transform probing                        |         |       |  3:06 |   5.2 |
| COMPLETED Catch directory generation errors in probe                        |         |       |  1:05 |   1.8 |
| COMPLETED Remove dumping of models from log                                 |         |       |  0:06 |   0.2 |
| COMPLETED Move model sorter from helpers                                    |         |       |  0:32 |   0.9 |
| COMPLETED Add canonical archetype support to yarn                           |         |       |  7:18 |  12.3 |
| COMPLETED Remove artefact from assistant casting                            |         |       |  0:20 |   0.6 |
| COMPLETED Untabify windows templates                                        |         |       |  0:04 |   0.1 |
| COMPLETED Output stats in org-mode format                                   |         |       |  0:40 |   1.1 |
| COMPLETED Split out transforms from post-processing chain                   |         |       |  4:19 |   7.3 |
| POSTPONED Move enablement into yarn                                         |         |       |  4:50 |   8.2 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2017-08-30 Wed 22:57]
    CLOCK: [2017-08-30 Wed 22:13]--[2017-08-30 Wed 22:57] =>  0:44

Add github release notes for previous sprint.

Title: Dogen v1.0.04, "Zona dos Riscos"

#+begin_src markdown
![Zona dos Riscos](http://www.almadeviajante.com/wp-content/uploads/deserto-do-namibe.jpg)
_Zona dos Riscos, Namibe, Angola. (C) Alma de Viajante, 2017._

Overview
=======
As usual, yarn internal refactoring is the bulk of the work in this sprint. The refactoring work had three major themes:

- **Use shared pointers across the board** for yarn elements, from frontend to the backend. This was done as a requirement for the exogenous models changes described below; as it happens, it has the nice side-effect of reducing the number of copies of model elements.
- **Finish exogenous models support**: frontends now have a special purpose model type, designed only for the kind of operations supported at the frontend level. This cleaned up transformations quite a bit, making it obvious which ones apply at which stage. The conceptual model is now somewhat cleaner, with the introduction of _exomodels_ (previously "exogenous models") and _endomodels_ (previously "intermediate models"), which specific purposes.
- **Name processing now done in core**: as part of the exogenous models change, we also moved the external and model module processing away from the frontends and into the core. This means less code duplication across frontends.

In addition to these, there were a couple of additional stories that had user facing impact, described in the next section.

User visible changes
================
This sprint introduced a number of user visible changes, all related to the internal clean-up work:

- Upsilon support was considered deprecated, since the customer for which we developed it no longer requires it. Since it was a custom-made frontend with no real application outside of this specific use case, all code related to upsilon has been removed.
- Continuing the meta-name work, JSON now represents these as regular yarn names. Sadly this makes the JSON more verbose, but at least it's more consistent now. This change breaks backwards compatibility, so users with JSON models need to update them. Sample change:
```
-    "meta_type": "module",
+    "meta_name": {
+      "simple": "module",
+      "external_modules": "dogen",
+      "model_modules": "yarn",
+      "internal_modules": "meta_model"
+    }
```
- A new command line flag was introduced: ```--compatibility-mode```. The objective of this flag is to disable some of the model validation code, where the errors are known to be caused by a forwards or backwards incompatible change. However: a) this is an experimental flag, very incomplete at present; and b) even when finished, the generated code may just be invalid.

For more details of the work carried out this sprint, see the [sprint log](https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_04.org).

Next Sprint
===========
Next sprint we'll resume the work on moving kernel-agnostic transformations from the kernels into yarn, and start looking at the meta-data/concepts clean-up.

Binaries
======
You can download binaries from [Bintray](https://bintray.com/domaindrivenconsulting/Dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.04_amd64-applications.deb](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen_1.0.04_amd64-applications.deb)
- [dogen-1.0.04-Darwin-x86_64.dmg](https://dl.bintray.com/domaindrivenconsulting/Dogen/1.0.04/dogen-1.0.04-Darwin-x86_64.dmg)
- [dogen-1.0.04-Windows-AMD64.msi](https://dl.bintray.com/domaindrivenconsulting/Dogen/dogen-1.0.04-Windows-AMD64.msi)

**Note**: They are produced by CI so they may not yet be ready.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/903140257218088960][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6308906359798657024/][LinkedIn]]
- [[https://gitter.im/DomainDrivenConsulting/dogen][Gitter]]

*** COMPLETED Sprint and product backlog grooming                     :story:
    CLOSED: [2017-09-18 Mon 00:04]
    CLOCK: [2017-09-15 Fri 18:40]--[2017-09-15 Fri 18:50] =>  0:10
    CLOCK: [2017-09-10 Sun 00:19]--[2017-09-10 Sun 00:30] =>  0:11
    CLOCK: [2017-09-08 Fri 13:45]--[2017-09-08 Fri 13:53] =>  0:08
    CLOCK: [2017-09-02 Sat 12:45]--[2017-09-02 Sat 13:14] =>  0:29
    CLOCK: [2017-09-01 Fri 15:52]--[2017-09-01 Fri 16:07] =>  0:15
    CLOCK: [2017-08-30 Wed 22:58]--[2017-08-30 Wed 23:25] =>  0:27
    CLOCK: [2017-08-30 Wed 22:09]--[2017-08-30 Wed 22:12] =>  0:03

Updates to sprint and product backlog.

*** COMPLETED Remove object types in yarn                             :story:
    CLOSED: [2017-08-31 Thu 08:58]
    CLOCK: [2017-08-31 Thu 08:32]--[2017-08-31 Thu 09:01] =>  0:29
    CLOCK: [2017-08-31 Thu 08:08]--[2017-08-31 Thu 08:31] =>  0:23

We need to figure out if this enumeration is still in use and if not
what needs to be done to remove it.

Seems like we are only using associative container at present. We
could probably replace the enumeration with a simple flag

*** COMPLETED Clean-up readme                                         :story:
    CLOSED: [2017-09-01 Fri 11:09]
    CLOCK: [2017-09-01 Fri 11:19]--[2017-09-01 Fri 11:28] =>  0:09
    CLOCK: [2017-09-01 Fri 11:10]--[2017-09-01 Fri 11:18] =>  0:08
    CLOCK: [2017-09-01 Fri 10:11]--[2017-09-01 Fri 11:09] =>  0:58

There are a number of minor changes that need to be done to the readme
file:

- fix typos
- bintray binaries are no longer experimental as we've been using them
- make build instructions a bit less repetitive

*** COMPLETED Rename transformers to adapters                         :story:
    CLOSED: [2017-09-01 Fri 11:55]
    CLOCK: [2017-09-01 Fri 11:48]--[2017-09-01 Fri 11:55] =>  0:07
    CLOCK: [2017-09-01 Fri 11:45]--[2017-09-01 Fri 11:47] =>  0:02
    CLOCK: [2017-09-01 Fri 11:29]--[2017-09-01 Fri 11:44] =>  0:15
    CLOCK: [2017-08-31 Thu 09:02]--[2017-08-31 Thu 09:06] =>  0:04

In the past we used the term "transformer" to mean a class that
converts types from one representation to another. However, now that
we are using domain terminology, the term "transforms" is taken to
mean a model transformation. To avoid confusion we should rename the
existing transformers to converters, adapters or some other
out-of-the-way name.

Affected models:

- quilt.cpp
- quilt.csharp
- yarn.dia

*** COMPLETED Analysis on annotations and profiles                    :story:
    CLOSED: [2017-09-02 Sat 13:06]
    CLOCK: [2017-09-01 Fri 15:25]--[2017-09-01 Fri 15:51] =>  0:26
    CLOCK: [2017-09-01 Fri 11:56]--[2017-09-01 Fri 13:56] =>  2:00

We can broadly divide annotations into three parts:

- *The annotations type system*. This is at present done by loading type
  templates. The right thing to do is to allow each model to create
  annotation types; these are then code-generated into a class which
  returns a list of type templates. Yarn needs to have a registrar for
  the type templates, which is populated during
  initialisation. Context factory talks to the registrar to obtain the
  type templates and initialises the type repository with it. The key
  thing is that the type system is static, and is more or less only a
  dogen concern (albeit users can define and consume their types, via
  the registrar). Thus we can quite easily solve the problems with the
  type system.
- *The templating machinery*. Annotations profiles are, on the main, a
  way to dynamically introduce annotation templates. These are
  "dynamic" because it is conceivable that each user will want to
  create its own set of annotation templates. There are two use cases:
  a) a common set of profiles, reused by several models (e.g. enable
  all facets, etc) b) a specific set of profiles useful only for one
  model (e.g. c++ artefact formatter). The profiling machinery
  requires a bit more thinking.
- *The configuration machinery*: the final piece of the puzzle is
  reading out data from annotations and using it in C++ code. For this
  we have three components: a) the type group classes, which aggregate
  the required annotation types b) the "configuration" classes, which
  are strongly typed representations of data stored in
  annotations; and c) a factory class responsible for using the type
  group class to populate the corresponding configuration class. In
  most simple cases, we could automate the generation of this triplet
  of classes.

The templating machinery is the most complex side of annotations then.
However, as it turns out, plain UML machinery can be used to handle
annotations profiles: UML profiles and UML stereotypes. Let's first
look into how we use stereotypes. We have:

- *Hard-coded static stereotypes*, which are in effect ways to map
  yarn meta-types into UML. For these we can simply hard-code the
  values and not worry about it. This is the right thing to do because
  the meta-types will not change often and when they do it will
  require a lot of manual work in yarn and in the frontends.
- *Dynamic stereotypes*. At present, this is how annotation profiles
  bind to element instances. We can make use of the labels in the
  annotation profile and refer to it in an element. During annotations
  transform, we expand the stereotype to the profile.

We can tackle this problem as follows:

- create a new element called =stereotype= (or perhaps
  =meta_stereotype=?). It has attributes which contain all of the
  required properties to create annotation profiles.
- create a new reference type called =profiles=. Problem: at present
  references are supplied by meta-data in target; however, we must
  process the profiles before reading in the target. The command line
  option solves this problem, at the cost of creating an inconsistency
  between references and profiles.
- update the exogenous model chain with a "profile" mode. In this
  case, the exogenous model can only contain =stereotypes=. No other
  UML stereotype is allowed. Create a context with an annotations
  group factory that loads no annotations profiles.
- create a transform that takes in the stereotypes and produces
  annotation profiles.
- create a second context based on the first one, but using a
  annotation group factory populated with all the annotation
  profiles. This will be the final context, used for all models.
- For each exogenous model: if the model contains stereotypes,
  transform them into annotation profiles before performing the
  annotations transform. This allows each model to supply its local
  stereotypes, visible only to the model. Only profile models have
  global stereotypes.

This would all be made easier if somehow it was possible to provide
annotation profiles externally to the context; this way we could
supply them globally or locally (ideally both), just before we do the
annotations transform. We need to look into changing the annotations
group factory API to cope with this.

Merged stories:

*Investigate code-generation around annotations*

We have two cases where code-generation makes sense for
annotations. Let look at them in turn.

Type templates

At present we are supplying JSON files with type templates. In truth
these are not really "data files" because changing them will cause
problems to the system; its tightly coupled to them. It would make
more sense to allow models to define their type templates inside the
model itself. We could use a stereotype of
=annotations::type_template= and then use meta-data for all of the
fields, as per JSON, e.g.:

:  {
:    "name": {
:      "simple": "profile",
:      "qualified": "annotations.profile"
:    },
:    "archetype_location": {
:      "family": "annotations"
:    },
:    "value_type": "text",
:    "template_kind": "instance",
:    "scope": "any"
:  }

We then code-generate the insertion of the type template into the
annotations type templates repository via an initialisers-like
framework.

It may make more sense to have one UML class with all the type
templates; the type templates then become attributes of that
class. The problem then is what to name that class. also, we may want
to have a couple of these, to group type templates logically (for
example we want the top-level templates like =enabled= separated from
the namespace-specific templates).

But the gist of it is that its very straightforward to add some
machinery that generates the code required to inject the type
templates into the system, and that it is triggered during
initialisation, replacing JSON loading.

Use of annotations

We then have the following usage pattern:

- define a class with all the related fields (with types of the type
  templates above). We call this class =type_group=. We may need to
  instantiate it for specific fields, or by facet, etc. We need to
  look at all of the examples in the code-base. Note that the layout
  of this class will (likely) bear no resemblance to the type
  templates grouping - this is just a "bag" with all of the available
  type templates, whereas the type group aggregation does normally
  have some useful meaning (e.g. =orm_properties=, etc).
- define a "factory" class for the type group class that uses the
  traits to locate the types (instances of type templates). For this,
  the type group class attributes need to refer to the fully qualified
  field name (possibly requiring some inputs such as kernel, facet).
- define a c++ class with the properties we're interested in. We
  normally call this class =_configuration= if its just used to read
  the meta-data, or =_properties= if its used as a real type. Note
  that at present we have allowed the layout of the type group class
  and the properties/configuration classes to be possibly quite
  different; we gather _all_ of the types of interest in the type
  group class, but then have multiple properties/configuration classes
  to match our needs.
- finally, we define a "factory" class that takes in the type group
  and produces the configuration/properties class.

In a code-generated world:

- we need to somehow force the type group class to match the
  configuration class; this will probably result on a lot of
  duplication. For example, for the ORM properties, we probably have a
  couple in common across object/model/attributes.
- we need to map C++ types into annotation types such that we can back
  out the annotation type from a c++ type. For example, given an
  enumeration, we want to create a annotation type of "text" but then
  automatically generate the "from/to" converters for the enumeration.

*Code generation of dynamic instances*

We seem to have a pretty well established usage pattern for dynamic,
so it may be a candidate for code generation. All we need is:

- a stereotype to mark a class as dynamic; the attributes of the class
  are dynamic fields, and their types must be one of the valid values
  for dynamic fields. The default value is used for the field's
  default value. Qualified name, ownership hierarchy, definition type,
  scope, etc are supplied as meta-data.
- stereotype name should be something like =DynamicFieldGroup=.
- the injection of the settings class is done by looking at the
  =DynamicFieldGroup= class and mapping the dynamic types to C++
  types. Note: this mapping should be dynamic too so that we can use
  it for other languages. We just need a meta-data tag for this, like
  we do with default enum value.
- the injection of the settings factory class is a bit more
  complicated; we need to mark the object as a settings factory. At
  present we have object types, but it was supposed to be removed
  after a refactoring. Actually we just need to create a new kind of
  element (=dynamic_settings_factory=?). In addition, settings factory
  may also need to take in some parameters such as facet/formatter.
- a stitch template that generates the settings factory.
- a stitch template that registers the dynamic field definition;
  instead of JSON we can just generate c++ code to perform the
  injection.
- we could also generate the repository and in most cases the
  repository factory. The only case where this breaks down is when we
  need to look at properties too.
- we should have a number of knobs to control generation: a) generate
  field injection b) generate settings factory c) generate repository
  d) generate repository factory.

We also need to merge the traits class directly into the factory. In
the majority of cases, we have traits just to access the fields. But
there are a few cases where we use traits for other purposes such as
formatter naming.

*Add support for "one off" profiles*

At present one can define top-level profiles. These are useful, but in
practice we ended up still defining a lot of things in each model. We
need a way to associate a profile with a model by supplying it on the
command line. That way users can create profiles and store them next
to the model rather than having to create a data directory, etc etc.

Actually the problem is that profiles aren't really implemented
correctly. First we should not call them profiles at all since they
are not UML profiles and overloading the term just generates
confusion. Second its important to understand how Dogen profiles come
about:

- we extend the UML meta-model via stereotypes to support all of the
  required yarn and quilt concepts.
- when we instantiate the yarn/quilt types via a UML model, we need to
  supply the values for the attributes which have been extended. If
  done properly this would happen via UML tagged values. Dia does not
  support these. At any rate, at present we use Dogen meta-data which
  is almost like tagged values.
- Dogen profiles are then an attempt to create bundles of tagged
  values with pre-populated values so that we do not need to manually
  populate them for every type. Instead, we can associate a stereotype
  with the type and then the system will automatically populate the
  values from the bundle.
- From all of this it follows that it should be possible to define
  these "bundles" directly in a UML diagram. If we were to use UML
  properly (or at least almost properly), we would define a class with
  a stereotype of =stereotype=, a name of the stereotype we'd like to
  define (say =Serializable=) and then its tagged values are the keys
  and values of the meta-data we want to define. This is strictly
  speaking not correct UML because we are stating we are augmenting
  the UML meta-model (hence =stereotype=) but then we end up
  instantiating a meta-model class with some predefined values. Its
  not clear how to express this in UML. Note that we have exactly the
  same issue with concepts.
- and, after some thinking, we are trying to do exactly the same thing
  as we are already doing for concepts: i.e. some kind of meta-level
  operation that allows us to add structural features to an
  element. Thus we can just use concepts, which are not even defined
  in UML - augmenting its meaning will not take us away from the
  literature. We can very simply add a last step to concepts transform
  which merges the annotations of the concept objects, using exactly
  the machinery we defined for profiles. The only slight problem is
  that we cannot reuse concepts across models.

Tasks:

- add annotations merging to concepts processing. Should cause no
  changes at all on all models.
- create a model in dogen defining basic concepts.

Links:

- [[https://msdn.microsoft.com/en-us/library/dd465146.aspx][Standard stereotypes for UML models]]

*** COMPLETED Use namespaced stereotypes                              :story:
    CLOSED: [2017-09-03 Sun 18:55]
    CLOCK: [2017-09-03 Sun 17:39]--[2017-09-03 Sun 18:54] =>  1:15
    CLOCK: [2017-09-02 Sat 20:35]--[2017-09-02 Sat 21:09] =>  0:34

Originally we added a space in the ORM stereotypes:

: orm value

This is not a particularly good idea. We should just add support for
namespaced stereotypes:

: orm::value

We should also change all of the existing stereotypes to have a
namespace:

: modeling::object

And so forth. The namespace name probably needs a bit of thinking.

Actually, we should name all of the static stereotypes with a
namespace, and making it clear they are connected to yarn. Example:

: yarn::enumeration
: yarn::orm::value

and so forth.

*** COMPLETED Tailor dogen models                                     :story:
    CLOSED: [2017-09-08 Fri 09:23]
    CLOCK: [2017-09-08 Fri 09:02]--[2017-09-08 Fri 09:23] =>  0:21

We are making a lot of changes to the JSON frontend, but our tests are
not exactly comprehensive. It would be good to start tailoring the
dogen models too, just to see what changes - even if we know we cannot
knit them yet.

The only snag is that we broke indent all json for now, but we'll have
to live with it. Northwind model seems borked.

*** COMPLETED Rename yarn concepts                                    :story:
    CLOSED: [2017-09-08 Fri 13:51]
    CLOCK: [2017-09-08 Fri 13:15]--[2017-09-08 Fri 13:45] =>  0:30
    CLOCK: [2017-09-08 Fri 11:48]--[2017-09-08 Fri 12:01] =>  0:13
    CLOCK: [2017-09-08 Fri 08:56]--[2017-09-08 Fri 09:01] =>  0:05
    CLOCK: [2017-09-08 Fri 08:25]--[2017-09-08 Fri 08:55] =>  0:30
    CLOCK: [2017-09-07 Thu 20:05]--[2017-09-07 Thu 20:54] =>  0:49
    CLOCK: [2017-09-07 Thu 08:53]--[2017-09-07 Thu 08:57] =>  0:04
    CLOCK: [2017-09-07 Thu 08:39]--[2017-09-07 Thu 08:53] =>  0:14
    CLOCK: [2017-09-07 Thu 08:32]--[2017-09-07 Thu 08:38] =>  0:06
    CLOCK: [2017-09-07 Thu 08:05]--[2017-09-07 Thu 08:31] =>  0:26
    CLOCK: [2017-09-05 Tue 22:55]--[2017-09-05 Tue 23:32] =>  0:37
    CLOCK: [2017-09-05 Tue 21:51]--[2017-09-05 Tue 22:54] =>  1:03
    CLOCK: [2017-09-05 Tue 21:39]--[2017-09-05 Tue 21:50] =>  0:11
    CLOCK: [2017-09-05 Tue 20:51]--[2017-09-05 Tue 21:39] =>  0:48
    CLOCK: [2017-09-05 Tue 18:43]--[2017-09-05 Tue 18:54] =>  0:11
    CLOCK: [2017-09-05 Tue 18:06]--[2017-09-05 Tue 18:42] =>  0:36
    CLOCK: [2017-09-05 Tue 07:35]--[2017-09-05 Tue 08:20] =>  0:45
    CLOCK: [2017-09-04 Mon 21:58]--[2017-09-04 Mon 22:39] =>  0:41

When concepts were introduced, it was clear that something was not
quite right on the naming. We used the C++ terminology because it
mapped well enough to the idea, but it was understood that we were
talking about two different things. As part of the clean-up required
for profiles, its time to revisit concepts.

A yarn concept is, really, an "object template". That is, it allows us
to create a template of a subset of the structure of a yarn object,
which can then be instantiated (pasted?) into actual objects. One
would like to avoid the use of the word "template", due to its C++
connotations, but sadly it seems there isn't a more appropriate word.

Unlike objects, object templates support multiple inheritance.

We consume the object templates via stereotypes.

Thus so far we should just rename concepts to object templates.

However, the downside is that we now have a verbose stereotype:

: object_template
: yarn::object_template

Having said that, we cannot have values for all attributes in an
object, just the attribute collection.

Tasks:

- rename yarn element and model collections.
- rename object's modeled concepts
- rename concept transform, tests
- rename meta-name, update JSON for tests
- tidy-up mock factory (variables, method names)
- rename stereotype
- rename test model types with concept in the name.
- check that tailor generates correct code.
- ORM transform uses expand instead of transform

*** COMPLETED Remove extra copying in mapper                          :story:
    CLOSED: [2017-09-08 Fri 13:59]
    CLOCK: [2017-09-08 Fri 13:54]--[2017-09-08 Fri 13:59] =>  0:05

It seems in addition to cloning the model in mapper, we are also
copying the objects. This seems wrong. Try not copying and see what
happens.

*** COMPLETED Investigate the JSON break on Northwind model           :story:
    CLOSED: [2017-09-08 Fri 14:58]
    CLOCK: [2017-09-08 Fri 14:32]--[2017-09-08 Fri 14:58] =>  0:26
    CLOCK: [2017-09-08 Fri 14:00]--[2017-09-08 Fri 14:31] =>  0:31

At present we cannot indent all JSON documents because one of the
models does not validate. It must be a tailor bug. Fix it.

The problem is with ODB pragmas:

: "odb_pragma" : "column("LASTNAME")"

We need to somehow escape the quotes. However, why do we even need to
have these set? Actually these are required because we are renaming
the field (from =last_name=).

*** COMPLETED Rename ODB parameters                                   :story:
    CLOSED: [2017-09-08 Fri 15:10]
    CLOCK: [2017-09-08 Fri 14:59]--[2017-09-08 Fri 15:10] =>  0:11

At present we use the following form:

: #DOGEN odb_pragma=no_id

Finally we should no longer attempt to derive the ODB pragma
context. We should just add it verbatim.

We need to use the new naming style =quilt.cpp.odb.pragma=. We also need to
rename the opaque_parameters to reflect ODB specific data.

*** COMPLETED Investigate usage of origin type                        :story:
    CLOSED: [2017-09-08 Fri 22:50]
    CLOCK: [2017-09-08 Fri 22:35]--[2017-09-08 Fri 22:50] =>  0:15

With the current setup of the transforms, we always know who the
target model is. Thus the =origin_types= flag may not be used
correctly at present.

Actually, we need way to figure out which types to generate, and which
types are references. So we still need this flag.

*** COMPLETED Improve dumping of models                               :story:
    CLOSED: [2017-09-13 Wed 21:41]
    CLOCK: [2017-09-13 Wed 19:25]--[2017-09-13 Wed 21:41] =>  2:16
    CLOCK: [2017-09-12 Tue 20:25]--[2017-09-12 Tue 21:40] =>  1:15
    CLOCK: [2017-09-12 Tue 07:35]--[2017-09-12 Tue 08:17] =>  0:42
    CLOCK: [2017-09-11 Mon 20:42]--[2017-09-11 Mon 22:54] =>  2:12
    CLOCK: [2017-09-11 Mon 19:20]--[2017-09-11 Mon 20:41] =>  1:21
    CLOCK: [2017-09-10 Sun 18:58]--[2017-09-10 Sun 19:33] =>  0:35
    CLOCK: [2017-09-10 Sun 18:31]--[2017-09-10 Sun 18:57] =>  0:26
    CLOCK: [2017-09-10 Sun 18:25]--[2017-09-10 Sun 18:30] =>  0:05
    CLOCK: [2017-09-10 Sun 15:27]--[2017-09-10 Sun 16:37] =>  1:10
    CLOCK: [2017-09-10 Sun 15:14]--[2017-09-10 Sun 15:26] =>  0:12
    CLOCK: [2017-09-10 Sun 14:49]--[2017-09-10 Sun 15:13] =>  0:24
    CLOCK: [2017-09-10 Sun 14:35]--[2017-09-10 Sun 14:48] =>  0:13
    CLOCK: [2017-09-10 Sun 11:49]--[2017-09-10 Sun 13:04] =>  1:15
    CLOCK: [2017-09-10 Sun 10:55]--[2017-09-10 Sun 11:48] =>  0:53

At present it is very difficult to find the log information regarding
models at different stages in the pipeline. It seems we are reaching
the limits for what logging can do for us here:

- the models are so large even emacs is struggling with the long line
  sizes.
- if we dump all models, we end up with extremely large log files. But
  in practice we tend to be looking for specific dumps: a model at
  transformation x or between x and y.

It would be much better if:

- there was a command line option that triggered the saving of
  models to a user supplied directory.
- we came up with a directory/file structure that allowed one to
  quickly find the model one is after. For example, the nesting of
  transformations could be the folder structure.

To start off with we could dump all models for all transformations to
get us up and running quickly, but in an ideal world we should be able
to supply the stages/transformations for which we want dumps. We can
then take the dumps and diff them from emacs.

The folder structure could also include a "before" and "after" for
each transform. Disk space is not an issue given that we would only be
using this when things went wrong. It also means we can save the files
as formatted JSON given that new lines are no longer an issue as they
are with the log file.

Similar to the context class, we could create a dumping context that
keeps track of the nesting of transforms and their names and knows if
dumping is enabled.

For tests we should default to the directory of the logs. It should be
possible to enable this feature for just one test, given that we will
generate very large amounts of data.

In a service setup we need a way for this data to be pushed somewhere
else like a cache, but this will have to wait until there is clarity
on just how IO will be implemented.

Notes:

- create a =probe= class that is supplied to all transforms. It is
  const; any changeable state is marked as mutable, since it is
  morally const.
- If probing is off, nothing happens when we call methods of this
  class.
- a probe supports the following operations: start/finish chain,
  start/finish transform. These pairs are called scopes.
- Start operations take:
  - a name which is the id of the transform. All transforms must now
    have an ID.
  - input: one of the  three kinds of models plus code generation
    output for the model to text transforms.
- Finish operations take:
  - output.
- each scope is logged as a guid. This makes it easier to correlate
  the dumps with the log file.
- the probe is initialised with a directory where all files will be
  dumped.
- every time we enter a chain scope, we create a new directory.
- every time we enter a transform scope we bump the transform counter
  by one. When we dump inputs and outputs, we write the files as:

: [TRANSFORM_COUNTER]-[TRANSFORM_ID]-[GUID]-[input|output].json
: 001-yarn.transforms.some_transform-e9d67262-f8f6-4291-a259-ebabe89b217a-input.json
: 001-yarn.transforms.some_transform-e9d67262-f8f6-4291-a259-ebabe89b217a-output.json

- command line arguments are:

: transforms-probe-stats
: transforms-probe-stats-graph
: transforms-probe-all
: transforms-probe-directory

- if =transform-print-stats= is enabled, a report is generated with
  the transform graph and the total execution time taken by each
  transform and chain. This is written in JSON for easy diffing.
- must ensure the dump stats output states:
  - if build is debug or release
  - version
  - log level
  - if dump data is enabled

Links:

- [[http://www.randygaul.net/2015/06/15/printing-pretty-ascii-trees/][Printing Pretty Ascii Trees]]

*** COMPLETED Create yarn options                                     :story:
    CLOSED: [2017-09-13 Wed 22:53]
    CLOCK: [2017-09-13 Wed 21:55]--[2017-09-13 Wed 22:53] =>  0:58
    CLOCK: [2017-09-13 Wed 21:46]--[2017-09-13 Wed 21:54] =>  0:08

We need to replace the dependency on the options model and create a
stand alone yarn options class.

*** COMPLETED Merge knit with yarn                                    :story:
    CLOSED: [2017-09-13 Wed 23:31]
    CLOCK: [2017-09-14 Thu 06:21]--[2017-09-14 Thu 06:30] =>  0:09
    CLOCK: [2017-09-13 Wed 23:56]--[2017-09-14 Thu 00:03] =>  0:07
    CLOCK: [2017-09-13 Wed 23:32]--[2017-09-13 Wed 23:55] =>  0:23
    CLOCK: [2017-09-13 Wed 23:24]--[2017-09-13 Wed 23:31] =>  0:07
    CLOCK: [2017-09-13 Wed 23:17]--[2017-09-13 Wed 23:23] =>  0:06
    CLOCK: [2017-09-13 Wed 23:06]--[2017-09-13 Wed 23:16] =>  0:10
    CLOCK: [2017-09-13 Wed 22:54]--[2017-09-13 Wed 23:05] =>  0:11

There isn't a lot of reason to have a knit model, really, now that
yarn has taken over the model to text transforms. We should just move
housekeeper and the rest of knit into helpers and update the
code-generator to perform the complete workflow.

*** COMPLETED Add probing support to code generation tests            :story:
    CLOSED: [2017-09-15 Fri 10:36]
    CLOCK: [2017-09-15 Fri 08:21]--[2017-09-15 Fri 10:36] =>  2:15
    CLOCK: [2017-09-14 Thu 23:13]--[2017-09-14 Thu 23:43] =>  0:30

When we added probing support, we did it only for the knitter
binary. Where it is actually useful is with the unit tests so that we
can try to figure out what broke. Add probing support to yarn code
generation tests.

*** COMPLETED Find some way of scoping transform probing              :story:
    CLOSED: [2017-09-15 Fri 13:43]
    CLOCK: [2017-09-15 Fri 10:37]--[2017-09-15 Fri 13:43] =>  3:06

At present we are manually calling start/end transform. This is not
ideal because:

- we may forget to call it;
- we may not look carefully at all the code paths; an early return
  will cause mismatches;
- we don't cope with exceptions.

Scope guard/RAII was designed exactly for this. However, the problem
we have is that we have a number of different APIs for start/end
transform and it will be difficult to create a scope guard that can
cater for all the permutations. We should classify the permutations
and see if this is doable, even if we need several scope guards.

We need:

- transform description
- transform id
- model id (optional)
- input (optional)
- output (optional); triggers dismissal

If output is not set we simply call end.

We need a scope for chains and another for transforms.

*** COMPLETED Catch directory generation errors in probe              :story:
    CLOSED: [2017-09-15 Fri 15:35]
    CLOCK: [2017-09-15 Fri 15:01]--[2017-09-15 Fri 15:39] =>  0:34
    CLOCK: [2017-09-15 Fri 14:33]--[2017-09-15 Fri 15:00] =>  0:27

Our paths are too deeply nested for windows machines, where there is a
limitation on the maximum path size. When that happens we throw a ios
error, which is not particularly informative. We should catch the
exception and add a message asking the user to disable probing data.

We could also perhaps have a mode with only directory numbers for
windows so that we can at least use probing.

Actually it seems the error is writing the file, not creating the
directory since that is all covered by exceptions.

*** COMPLETED Remove dumping of models from log                       :story:
    CLOSED: [2017-09-15 Fri 15:53]
    CLOCK: [2017-09-15 Fri 15:47]--[2017-09-15 Fri 15:53] =>  0:06

*Rationale*: we didn't see any obvious places where this is happening.

Once probing is in place, we need to remove all the dumps we are doing
of models at present. These just add size to the log files for no
reason.

*** COMPLETED Move model sorter from helpers                          :story:
    CLOSED: [2017-09-15 Fri 17:22]
    CLOCK: [2017-09-15 Fri 17:12]--[2017-09-15 Fri 17:29] =>  0:17
    CLOCK: [2017-09-15 Fri 16:04]--[2017-09-15 Fri 16:10] =>  0:06
    CLOCK: [2017-09-15 Fri 15:54]--[2017-09-15 Fri 16:03] =>  0:09

This is really just a sorting transform, not a helper.

*** COMPLETED Add canonical archetype support to yarn                 :story:
    CLOSED: [2017-09-16 Sat 00:24]
    CLOCK: [2017-09-15 Fri 23:34]--[2017-09-16 Sat 00:07] =>  0:33
    CLOCK: [2017-09-10 Sun 00:08]--[2017-09-10 Sun 00:18] =>  0:10
    CLOCK: [2017-09-09 Sat 23:40]--[2017-09-10 Sun 00:07] =>  0:27
    CLOCK: [2017-09-09 Sat 23:28]--[2017-09-09 Sat 23:39] =>  0:11
    CLOCK: [2017-09-09 Sat 22:50]--[2017-09-09 Sat 23:27] =>  0:37
    CLOCK: [2017-09-09 Sat 22:05]--[2017-09-09 Sat 22:49] =>  0:44
    CLOCK: [2017-09-09 Sat 21:02]--[2017-09-09 Sat 21:29] =>  0:27
    CLOCK: [2017-09-09 Sat 19:52]--[2017-09-09 Sat 20:15] =>  0:23
    CLOCK: [2017-09-09 Sat 19:12]--[2017-09-09 Sat 19:51] =>  0:39
    CLOCK: [2017-09-09 Sat 15:40]--[2017-09-09 Sat 16:08] =>  0:28
    CLOCK: [2017-09-09 Sat 13:49]--[2017-09-09 Sat 15:39] =>  1:50
    CLOCK: [2017-09-09 Sat 08:55]--[2017-09-09 Sat 09:27] =>  0:32
    CLOCK: [2017-09-08 Fri 22:51]--[2017-09-08 Fri 23:08] =>  0:17

We need to add a new attribute in context which captures the canonical
archetypes.

Notes:

- kernel must also return canonical archetype by element type
  index. Perhaps we should have a struct that aggregates both:
  archetype locations for meta-type? Or kernel can just return a
  =std::pair=.
- at present we have placed the canonical archetype resolution as part
  of the element properties. However, we do not need to have this at
  the element level since its a meta-type property and can be
  determined up-front. We do need to resolve a name into a meta-type
  before we can resolve a meta-type into a concrete archetype.
- we need to unpick the notion of whether a formatter is "includible"
  or not from the notion of canonical archetypes. Canonical archetypes
  is meta-model concept: given a facet and a meta-model type, which
  archetype represents the "key" definition of the element. It just so
  happens that this function has a use in identifying the files to
  include.
- before we focus too much on adding canonical archetype support to
  yarn, its important to understand just exactly how it gets used. We
  are doing far too many look-ups at present, given the information
  that is known. Canonical archetypes are a way to refer to a type for
  a given formatter without knowing who exactly that formatter
  is. This is useful for example when we need to include the
  definition of a type but we do not know if its an enum, object
  etc. So, at present, we proceed as follows:
  - for a given name, we first resolve the archetype; if its not
    canonical nothing happens. If its canonical, it gets resolved into
    a concrete archetype.
  - we then find the element associated with the name and get its
    properties. If the archetype is not enabled, there is nothing to
    do.
  - if its enabled, we then need to look for its directives group. If
    none exist, then there is nothing to do.

  The gist of this exercise is that we could get away with a single
  look-up; for this we would need to map the canonical directives
  group as well. Actually this will not work because an archetype can
  be disabled on one element but enabled on another, so we need to
  separate enablement from the directives group. But we certainly can
  map all the model elements to the meta-model elements and those to
  the canonical archetypes. So we can have an "enablement manager" of
  some kind, who consumes a model and the context, and creates a map
  of element and archetype to bool. Steps:
  - directive group repository factory needs to insert against the
    canonical artefact too.
  - dependencies builder needs to use the original (non-resolved)
    artefact name.
  - create a "enablement manager" in yarn that pre-processes the model
    and creates the da+ta structures as described above (for element
    id + archetype returns enabled flag)
  - supply the "enablement manager" to the kernel and from there to
    the dependencies builder.
  - at this point, all C++ specific enablement infrastructure can be
    deleted.

  An even simpler way of looking at this is to generate a set of pairs
  of strings during enablement transform for all elements + artefacts
  that are enabled; put that in the endomodel; merge the sets as part
  of the merger (throwing if duplicates are found). Then supply the
  set to the dependencies builder somehow (or create a helper in yarn
  that acts as the enablement manager but is just doing a look-up on
  the set).

Tasks:

- add class in annotations to generate the canonical form from a
  archetype location.
- update kernels to return archetype location groups, with canonical
  archetypes populated.
- add a set of pairs of strings to endomodel: enabled archetypes for
  element. Populate it during enablement transform. Add canonical
  archetype there too.
- in directive group repository factory, for each archetype that is
  the canonical archetype, populate its entry too. Remove use of the
  canonical resolver in dependencies builder.
- create a class to query the set: "enablement manager"? Instantiate
  it in C++ kernel and supply it to dependencies builder factory.
- remove all references of formattables and element properties in
  dependencies builder.

Notes:

- make is_enabled private in dependencies builder and see what breaks.

*** COMPLETED Remove artefact from assistant casting                  :story:
    CLOSED: [2017-09-16 Sat 18:38]
    CLOCK: [2017-09-16 Sat 18:18]--[2017-09-16 Sat 18:38] =>  0:20

At the moment we receiving an artefact parameter when casting yarn
elements via the assistant, but we don't do anything with it. Remove
it.

*** COMPLETED Untabify windows templates                              :story:
    CLOSED: [2017-09-16 Sat 18:43]
    CLOCK: [2017-09-16 Sat 18:39]--[2017-09-16 Sat 18:43] =>  0:04

It seems we have a lot of tabs on Visual Studio solutions, projects
etc. Untabify the templates and regenerate all.

*** COMPLETED Output stats in org-mode format                         :story:
    CLOSED: [2017-09-17 Sun 16:23]
    CLOCK: [2017-09-17 Sun 16:01]--[2017-09-17 Sun 16:22] =>  0:21
    CLOCK: [2017-09-17 Sun 15:43]--[2017-09-17 Sun 16:00] =>  0:17
    CLOCK: [2017-09-17 Sun 15:40]--[2017-09-17 Sun 15:42] =>  0:02

It would be nice to be able to collapse the stats sometimes, so one
can inspect the transform graph. However, we do not need this all the
time - the flat TXT format is useful too. We need a probing option to
output stats in org-mode format.

Also, we need to state that "debug" pertains to debug logging, not a
debug build. Add labels, e.g.: =log:debug=, etc.

*** COMPLETED Split out transforms from post-processing chain         :story:
    CLOSED: [2017-09-17 Sun 18:36]
    CLOCK: [2017-09-17 Sun 20:39]--[2017-09-17 Sun 20:50] =>  0:11
    CLOCK: [2017-09-17 Sun 19:11]--[2017-09-17 Sun 19:15] =>  0:04
    CLOCK: [2017-09-17 Sun 18:27]--[2017-09-17 Sun 18:35] =>  0:08
    CLOCK: [2017-09-17 Sun 18:15]--[2017-09-17 Sun 18:26] =>  0:11
    CLOCK: [2017-09-17 Sun 18:07]--[2017-09-17 Sun 18:14] =>  0:07
    CLOCK: [2017-09-17 Sun 16:56]--[2017-09-17 Sun 18:06] =>  1:10
    CLOCK: [2017-09-17 Sun 16:50]--[2017-09-17 Sun 16:55] =>  0:05
    CLOCK: [2017-09-17 Sun 16:42]--[2017-09-17 Sun 16:49] =>  0:07
    CLOCK: [2017-09-17 Sun 16:36]--[2017-09-17 Sun 16:41] =>  0:05
    CLOCK: [2017-09-17 Sun 16:23]--[2017-09-17 Sun 16:35] =>  0:12
    CLOCK: [2017-09-17 Sun 15:20]--[2017-09-17 Sun 15:39] =>  0:19
    CLOCK: [2017-09-17 Sun 15:00]--[2017-09-17 Sun 15:19] =>  0:19
    CLOCK: [2017-09-17 Sun 14:42]--[2017-09-17 Sun 14:59] =>  0:17
    CLOCK: [2017-09-17 Sun 14:31]--[2017-09-17 Sun 14:41] =>  0:10
    CLOCK: [2017-09-17 Sun 14:22]--[2017-09-17 Sun 14:30] =>  0:08
    CLOCK: [2017-09-17 Sun 14:05]--[2017-09-17 Sun 14:21] =>  0:16
    CLOCK: [2017-09-16 Sat 21:02]--[2017-09-16 Sat 21:32] =>  0:30

We have a lot of transforms in the post-processing chain. Many of
these do not actually require an endomodel; they are just traversing
all elements and doing something to them. We should look at all of the
transforms there and get rid of all of them that don't actually
require an endomodel. For these we should create a new chain that
operates on just models.

Notes:

- we need a good name for this chain since pre/post-processing are
  already pretty vague.
- we may need to still visit the elements.

Tasks:

- rename pre/post-processing chains to =endomodel_...=. Rename
  validators too.
- create a =model_postprocessing_chain= with all the transforms after
  =associations_transform=.
- create a model generation chain that owns the endomodel to model
  transform and the model post-processing pipeline.
- delete all the endomodel members that are no longer required; update
  adapter. Update yarn diagram.
- generability
- model to text rename

*** POSTPONED Move enablement into yarn                               :story:
    CLOSED: [2017-09-18 Mon 00:04]
    CLOCK: [2017-09-16 Sat 20:50]--[2017-09-16 Sat 21:01] =>  0:11
    CLOCK: [2017-09-16 Sat 17:43]--[2017-09-16 Sat 18:17] =>  0:34
    CLOCK: [2017-09-16 Sat 15:42]--[2017-09-16 Sat 16:53] =>  1:11
    CLOCK: [2017-09-16 Sat 15:25]--[2017-09-16 Sat 15:32] =>  0:07
    CLOCK: [2017-09-16 Sat 15:04]--[2017-09-16 Sat 15:24] =>  0:20
    CLOCK: [2017-09-16 Sat 14:54]--[2017-09-16 Sat 15:03] =>  0:09
    CLOCK: [2017-09-16 Sat 13:43]--[2017-09-16 Sat 14:53] =>  1:10
    CLOCK: [2017-09-16 Sat 12:45]--[2017-09-16 Sat 12:52] =>  0:07
    CLOCK: [2017-09-16 Sat 12:36]--[2017-09-16 Sat 12:44] =>  0:08
    CLOCK: [2017-09-16 Sat 09:04]--[2017-09-16 Sat 09:22] =>  0:18
    CLOCK: [2017-09-16 Sat 00:25]--[2017-09-16 Sat 00:28] =>  0:03
    CLOCK: [2017-09-16 Sat 00:08]--[2017-09-16 Sat 00:24] =>  0:16
    CLOCK: [2017-09-15 Fri 18:51]--[2017-09-15 Fri 19:07] =>  0:16

It seems that the concepts around enablement are actually not kernel
specific but instead can be generalised at the meta-model level. We
need to create adequate representations in yarn to handle facets,
etc. We then need to move across the code that computes enablement
into yarn so that all kernels can make use of it.

Problems:

- we are checking to see if the hash facet is enabled with c++ 98; if
  so, we throw as this facet is incompatible. We cannot do this from
  yarn since we do not know what c++ standards are.
- because we do not have a mapping between a archetype location and
  the meta-type, we will be enabling/disabling all archetype locations
  across all meta-types.
- because we do not have element segmentation, the element extensions
  will be disabled. Actually this will probably work just the same,
  given that all elements exist.
- enablement must be done after external transformations so it picks
  up fabric types.
- we need to support formatting styles in order to be able to use the
  artefact properties from the meta-model.
- in quilt.cpp, someone did an upfront generation of all archetype
  properties against the archetype locations. We not doing that in
  yarn, so nothing is coming out. This was done during transformation
  in formattables.
- with a move into yarn, we seem to have broken the overwrite flag
  logic; changes no longer result in new code being generated.
- we also have borked the includes: dependency builder is looking into
  the formattables instead of element. However, we then run into
  segmentation issues because we cannot find forward declarations on
  the main element.

To do:

- kernel registrar type index map - done.
- assistant to latch on to element; use new element properties where
  possible.
- facet properties must be handled, and assistant must use the yarn
  version.
- c# formatter registrar type index map - done.
- bug in template instantiating: artefact expansions do not seem to
  take kernel into account - done.
- use new enabled fields.
- delete all enablement classes in c++ and enabled/overwrite properties.

*Previous Understanding*

We need to make use of the exact same logic as implemented in
=quilt.cpp= for enablement. Perhaps all of the enablement related
functionality can be lifted and grafted onto quilt without any major
changes.

** Deprecated
*** CANCELLED Make the Zeta model compilable                          :story:
    CLOSED: [2017-08-30 Wed 23:01]

*Rationale*: not required since Upsilon has been deprecated.

We need to work through the list of issues with the Zeta model and get
it to a compilable state.

*** CANCELLED Registrar in quilt is not being generated               :story:
    CLOSED: [2017-08-30 Wed 23:14]

*Rationale*: quilt model has been deleted.

We don't seem to change the contents of this file when regenerating.

*** CANCELLED Stitcher log file names look weird                      :story:
    CLOSED: [2017-08-30 Wed 23:19]

*Rationale*: they look ok with the current release.

At present we are writing files with names like:

: dogen.stitcher...log

*** CANCELLED ODB options file is generated to incorrect location     :story:
    CLOSED: [2017-08-30 Wed 23:21]

*Rationale*: ODB options generation changed dramatically recently (one
per type, etc).

Models with composite names seem to have their ODB options file
generated under the =projects= directory, e.g.:

: projects/vtk/geometry/src/options.odb
